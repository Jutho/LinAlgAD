[
  {
    "objectID": "main.html",
    "href": "main.html",
    "title": "Automatic differentation rules for linear algebra",
    "section": "",
    "text": "$$ % Math operators % % \n% VECTORS\n% specifically for vectors in F^n \n% MATRICES\n% SCALARS \n% LINEAR MAPS\n% FIELDS \n% RELATIONS \n% % % % % % % % %\n%\n$$\nIn this document we derive the forward and reverse chain rule for a number of common matrix factorisations. In particular, we also focus partial or incomplete factorisations, as appear in Krylov algorithms, as well as on truncated (low-rank) factorisations. We first introduce some important concepts that will be used in several of those rules."
  },
  {
    "objectID": "main.html#complex-variables-and-chain-rules",
    "href": "main.html#complex-variables-and-chain-rules",
    "title": "Automatic differentation rules for linear algebra",
    "section": "Complex variables and chain rules",
    "text": "Complex variables and chain rules\nConsider a scalar-valued function \\(f\\), depending on complex parameters \\(z^k = x^k + {\\mathrm{i}}y^k\\), that is real-valued (and can thus not be holomorphic). Given a trajectory \\(z^k(t)\\) with derivatives \\(\\dot{z}^k = \\dot{x}^k + {\\mathrm{i}}\\dot{y}^k\\), we can write\n\\[\\begin{align}\n\\dot{f} &= \\sum_{k} \\frac{\\partial f}{\\partial x^k} \\dot{x}^k + \\frac{\\partial f}{\\partial y^k} \\dot{y}^k \\\\\n&= \\sum_{k,l} \\frac{\\partial f}{\\partial z^l} \\left(\\frac{\\partial z^l}{\\partial x^k} \\dot{x}^k + \\frac{\\partial z^l}{\\partial y^k} \\dot{y}^k\\right) + \\frac{\\partial f}{\\partial \\bar{z}^l} \\left(\\frac{\\partial \\bar{z}^l}{\\partial x^k} \\dot{x}^k + \\frac{\\partial \\bar{z}^l}{\\partial y^k} \\dot{y}^k\\right)\\\\\n&= \\sum_{k} \\frac{\\partial f}{\\partial z^k} \\dot{z}^k +  \\frac{\\partial f}{\\partial \\bar{z}^k} \\dot{\\bar{z}}^k\n\\end{align}\\] Here, we have that \\[\\begin{align}\n\\frac{\\partial f}{\\partial z^k} &= \\frac{1}{2}\\left(\\frac{\\partial f}{\\partial x^k} - {\\mathrm{i}}\\frac{\\partial f}{\\partial y^k}\\right)\\\\\n\\frac{\\partial f}{\\partial \\bar{z}^k} &= \\frac{1}{2}\\left(\\frac{\\partial f}{\\partial x^k} + {\\mathrm{i}}\\frac{\\partial f}{\\partial y^k}\\right) = \\overline{\\frac{\\partial f}{\\partial z^k}}\n\\end{align}\\] where the final equality only holds because \\(f\\) is assumed real. Typically, we can compute \\(\\frac{\\partial f}{\\partial z^k}\\) easily without explicitly going to real and imaginary components. We thus find \\[\\begin{align}\n\\dot{f} = \\sum_{k} \\frac{\\partial f}{\\partial z^k} \\dot{z}^k + \\overline{\\frac{\\partial f}{\\partial z^k}} \\overline{\\dot{z}^k}\n= 2 \\mathop{\\mathrm{Re}}\\left(\\sum_{k} \\frac{\\partial f}{\\partial z^k} \\dot{z}^k\\right)\n= 2 \\mathop{\\mathrm{Re}}\\left(\\sum_{k} \\overline{\\frac{\\partial f}{\\partial \\bar{z}^k}} \\dot{z}^k\\right)\n= 2 \\mathop{\\mathrm{Re}}\\left(\\boldsymbol{\\breve{z}}^\\dagger \\boldsymbol{\\dot{z}}\\right)\n\\end{align}\\] with thus \\(\\boldsymbol{\\dot{z}}\\) the vector with components \\(\\dot{z}^k\\) and \\(\\boldsymbol{\\breve{z}}\\) the vector with components \\(\\breve{z}_k=\\frac{\\partial f}{\\partial \\bar{z}^k}\\).\nWhen the coefficients \\(z^k \\cong Z^{(i,j)}\\) would be the components of a matrix \\(Z\\), we would rather write this as \\[\\begin{align}\n\\dot{f} = 2 \\mathop{\\mathrm{Re}}\\left(\\sum_{k} \\overline{\\frac{\\partial f}{\\partial \\bar{z}^k}} \\dot{z}^k\\right) = 2 \\mathop{\\mathrm{Re}}\\left(\\sum_{i,j} \\overline{\\frac{\\partial f}{\\partial \\bar{Z}^{i,j}}} \\dot{Z}^{i,j}\\right)\n= 2 \\mathop{\\mathrm{Re}}\\left(\\mathop{\\mathrm{Tr}}\\left[\\breve{Z}^\\dagger \\dot{Z}\\right]\\right) = 2\\mathop{\\mathrm{RTr}}\\left[\\breve{Z}^\\dagger \\dot{Z}\\right]\n\\end{align}\\] where I have introduced the new symbol \\(\\mathop{\\mathrm{RTr}}\\) for the real part of the trace.\nNow consider the composition where \\(f\\) is itself a function of complex parameters \\(\\boldsymbol{w}\\), which are themselves complex (not necessarily holomorphic) functions of other complex paramters \\(\\boldsymbol{z}\\). We can then write\n\\[\\begin{align}\n\\dot{f} = \\sum_{k,l} \\begin{bmatrix} \\overline{\\breve{w}_k} & \\breve{w}_k\\end{bmatrix}\\begin{bmatrix} \\frac{\\partial w^k}{\\partial z^l} & \\frac{\\partial w^k}{\\partial \\bar{z}^l}\\\\\n\\frac{\\partial \\bar{w}^k}{\\partial z^l} & \\frac{\\partial \\bar{w}^k}{\\partial \\bar{z}^l} \\end{bmatrix}\n\\begin{bmatrix} \\dot{z}^l \\\\ \\overline{\\dot{z}^l} \\end{bmatrix} = \\sum_{k,l} \\begin{bmatrix} \\breve{w}_k \\\\ \\overline{\\breve{w}_k}\\end{bmatrix}^\\dagger \\begin{bmatrix} \\frac{\\partial w^k}{\\partial z^l} & \\frac{\\partial w^k}{\\partial \\bar{z}^l}\\\\\n\\frac{\\partial \\bar{w}^k}{\\partial z^l} & \\frac{\\partial \\bar{w}^k}{\\partial \\bar{z}^l} \\end{bmatrix}\n\\begin{bmatrix} \\dot{z}^l \\\\ \\overline{\\dot{z}^l} \\end{bmatrix}\n\\end{align}\\] In this case (where \\(w\\) is complex), the entries of the Jacobian matrix are related via \\(\\overline{\\frac{\\partial w^k}{\\partial z^l}} = \\frac{\\partial \\bar{w}^k}{\\partial \\bar{z}^l}\\) and \\(\\overline{\\frac{\\partial w^k}{\\partial \\bar{z}^l}} = \\frac{\\partial \\bar{w}^k}{\\partial z^l}\\) Trying to rewrite the above expression in the standard form \\(2\\mathop{\\mathrm{Re}}(\\boldsymbol{\\breve{z}}^\\dagger \\boldsymbol{\\dot{z}})\\), we find\n\\[\\begin{align}\n\\dot{f} &= \\sum_{l} \\begin{bmatrix} \\left(\\sum_k \\overline{\\breve{w}_k} \\frac{\\partial w^k}{\\partial z^l} + \\breve{w}_k \\frac{\\partial \\bar{w}^k}{\\partial z^l}\\right) & \\left(\\sum_k \\overline{\\breve{w}_k} \\frac{\\partial w^k}{\\partial \\bar{z}^l} + \\breve{w}_k \\frac{\\partial \\bar{w}^k}{\\partial \\bar{z}^l}\\right) \\end{bmatrix} \\begin{bmatrix} \\dot{z}^l \\\\ \\overline{\\dot{z}^l} \\end{bmatrix} \\\\\n&= \\sum_{l} \\begin{bmatrix} \\sum_k \\breve{w}_k \\overline{\\frac{\\partial w^k}{\\partial z^l}} + \\overline{\\breve{w}_k} \\overline{\\frac{\\partial \\bar{w}^k}{\\partial z^l}} \\\\\n\\sum_k \\breve{w}_k \\overline{\\frac{\\partial w^k}{\\partial \\bar{z}^l}} + \\overline{\\breve{w}_k} \\overline{\\frac{\\partial \\bar{w}^k}{\\partial \\bar{z}^l}} \\end{bmatrix}^\\dagger \\begin{bmatrix} \\dot{z}^l \\\\ \\overline{\\dot{z}^l} \\end{bmatrix} \\\\\n&= \\sum_{l} \\begin{bmatrix} \\sum_k \\breve{w}_k \\frac{\\partial \\bar{w}^k}{\\partial \\bar{z}^l} + \\overline{\\breve{w}_k} \\frac{\\partial w^k}{\\partial \\bar{z}^l} \\\\\n\\sum_k \\overline{\\breve{w}_k} \\overline{\\frac{\\partial \\bar{w}^k}{\\partial \\bar{z}^l}} + \\breve{w}_k \\overline{\\frac{\\partial w^k}{\\partial \\bar{z}^l}}\\end{bmatrix}^\\dagger \\begin{bmatrix} \\dot{z}^l \\\\ \\overline{\\dot{z}^l} \\end{bmatrix} \\qquad \\begin{matrix}\\text{(by substituting ${\\overline{\\frac{\\partial \\bar{w}^k}{\\partial z^l}}}\\to\\frac{\\partial w^k}{\\partial \\bar{z}^l}$)}\\\\ \\text{(by switching the order of the two terms)}\\end{matrix}\\\\\n&= 2\\mathop{\\mathrm{Re}}\\left(\\sum_l \\left[\\sum_{k} \\breve{w}_k \\frac{\\partial \\bar{w}^k}{\\partial \\bar{z}^l} + \\overline{\\breve{w}_k} \\frac{\\partial w^k}{\\partial \\bar{z}^l}\\right]^\\dagger \\dot{z}^l\\right) = 2\\mathop{\\mathrm{Re}}(\\boldsymbol{\\breve{z}}^\\dagger\\dot{z})\n\\end{align}\\]\nwith thus \\(\\breve{z}_l = \\sum_k \\left[\\breve{w}_k \\frac{\\partial \\bar{w}^k}{\\partial \\bar{z}^l} + \\overline{\\breve{w}_k} \\frac{\\partial w^k}{\\partial \\bar{z}^l}\\right]\\). The same result would have been obtained from inserting \\(\\dot{w}^k = \\frac{\\partial w^k}{\\partial z^l}\\dot{z}^l + \\frac{\\partial w^k}{\\partial \\bar{z}^l}\\dot{\\bar{z}}^l\\) in \\(\\dot{f} = 2\\mathop{\\mathrm{Re}}(\\boldsymbol{\\breve{w}}^\\dagger \\boldsymbol{\\dot{w}})\\) as\n\\[\\begin{align}\n\\dot{f} &= 2\\mathop{\\mathrm{Re}}(\\boldsymbol{\\breve{w}}^\\dagger \\boldsymbol{\\dot{w}}) = 2\\mathop{\\mathrm{Re}}\\left(\\sum_k \\overline{\\breve{w}_k} \\dot{w}^k\\right)\\\\\n&= 2\\mathop{\\mathrm{Re}}\\left(\\sum_{k,l} \\overline{\\breve{w}_k} \\frac{\\partial w^k}{\\partial z^l} \\dot{z}^l\\right) + 2\\mathop{\\mathrm{Re}}\\left(\\sum_{k,l} \\overline{\\breve{w}_k} \\frac{\\partial w^k}{\\partial \\bar{z}^l} \\overline{\\dot{z}^l}\\right)\\\\\n&= 2\\mathop{\\mathrm{Re}}\\left(\\sum_{k,l} \\overline{\\breve{w}_k \\overline{\\frac{\\partial w^k}{\\partial z^l}}} \\dot{z}^l\\right) + 2\\mathop{\\mathrm{Re}}\\left(\\sum_{k,l} \\overline{\\overline{\\breve{w}_k} \\frac{\\partial w^k}{\\partial \\bar{z}^l}} \\dot{z}^l\\right)\n\\end{align}\\] where in the second term we have used that we can conjugate the whole argument of the \\(\\mathop{\\mathrm{Re}}\\) function. Hence, we again obtain\n\\[\\begin{align}\n\\dot{f} = 2\\mathop{\\mathrm{Re}}\\left(\\sum_{k,l} \\overline{\\left[\\breve{w}_k \\overline{\\frac{\\partial w^k}{\\partial z^l}}+\\overline{\\breve{w}_k} \\frac{\\partial w^k}{\\partial \\bar{z}^l}\\right]} \\dot{z}^l\\right)\n\\end{align}\\]\nwhich (not unexpectedly) yields the same result for \\(\\breve{z}_l\\), upon substituing \\(\\overline{\\frac{\\partial w^k}{\\partial z^l}} = \\frac{\\partial \\bar{w}^k}{\\partial \\bar{z}^l}\\). The reverse rules required by automatic differentation are exactly of the form that relate \\(\\breve{z}_l\\) to \\(\\breve{w}_k\\) (and possibly its conjugate). The way we derive them in practice is by starting from \\(\\mathop{\\mathrm{Re}}\\left(\\sum_k \\overline{\\breve{w}_k} \\dot{w}^k\\right)\\) and inserting the (easier) forward rule that expresses \\(\\dot{w}^k\\) in terms of \\(\\dot{z}^l\\) (and possibly its conjugate), and then using the fact that we can conjugate within the argument of \\(\\mathop{\\mathrm{Re}}\\) or \\(\\mathop{\\mathrm{RTr}}\\) in order to isolate the contribution that is contracted with \\(\\dot{z}^l\\)."
  },
  {
    "objectID": "main.html#matrices-and-their-tangents-and-cotangents",
    "href": "main.html#matrices-and-their-tangents-and-cotangents",
    "title": "Automatic differentation rules for linear algebra",
    "section": "Matrices and their tangents and cotangents",
    "text": "Matrices and their tangents and cotangents\nWe will be working with complex matrices \\(A \\in {\\mathbb{C}}^{m \\times n}\\). Sometimes these matrices satisfy additional constraints, which also constrains their tangents \\(\\dot{A}\\). Given that the constraints on the tangent vectors are always linear (in contrast to those on the variables themselves), we can typically satisfy them explicitly by choosing a proper parameterisation.\nConstraints on the parameters do not directly impose constraints on the adjoint/dual variables \\(\\breve{A}\\) (cotangents). However, in the way they couple to the tangents, they will be automatically projected into the relevant subspace.\nNote that the coupling between tangents and cotangents takes the form \\(\\mathop{\\mathrm{RTr}}(\\breve{A}^\\dagger \\dot{A})\\), which can be read as an inner product \\({\\left\\langle A,B\\right\\rangle} = \\mathop{\\mathrm{RTr}}(A^\\dagger B)\\). However, because we are taking the real part, this corresponds to treating the vector spaces \\({\\mathbb{C}}^{m \\times n}\\) of complex matrices as a real vector space. Indeed, the inner product can be rewritten as the standard (real) Euclidean inner product\n\\[{\\left\\langle A,B,=\\right\\rangle} \\sum_{i=1}^m\\sum_{j=1}^n \\mathop{\\mathrm{Re}}(A_{ij}) \\mathop{\\mathrm{Re}}(B_ij) + \\mathop{\\mathrm{Im}}(A_ij)\\mathop{\\mathrm{Im}}(B_ij).\\]\nThis has a number of interesting implications.\n\nDiagonal and triangular matrices\nThe easiest classes of special matrices are those where some of the entries are zero, such as diagonal or upper triangular matrices. Because of the structure of the inner product, only the corresponding entries of the dual variables will contribute. In fact, because of the structure of the real-valued inner product that we work with, we can even impose the real or imaginary component of an entry to be zero separately, and this information will also propagate onto the dual variables.\nWe can make this explicit by introducing specific projection (super)operators \\({\\mathbb{C}}^{m\\times n} \\to {\\mathbb{C}}^{m\\times n}\\) onto the relevant subspaces that we need below, namely \\[\\begin{align}\n\\mathcal{P}_L(A)_{k,l} &= \\begin{cases} A_{k,l},& k &gt; l\\\\ 0,& k\\leq l\\end{cases}\\\\\n\\mathcal{P}_U(A)_{k,l} &= \\begin{cases} A_{k,l},& k &lt; l\\\\ 0,& k\\geq l\\end{cases}\\\\\n\\mathcal{P}_{rD}(A)_{k,l} &= \\begin{cases} \\mathop{\\mathrm{Re}}(A_{k,k}),& k = l\\\\ 0,& k\\neq l\\end{cases}\\\\\n\\mathcal{P}_{iD}(A)_{k,l} &= \\begin{cases} {\\mathrm{i}}\\mathop{\\mathrm{Im}}(A_{k,k}),& k = l\\\\ 0,& k\\neq l\\end{cases}\\\\\n\\mathcal{P}_{D}(A)_{k,l} &= (\\mathcal{P}_{rD}+\\mathcal{P}_{iD})(A)_{k,l} \\\\\n&= \\begin{cases} A_{k,k},& k = l\\\\ 0,& k\\neq l\\end{cases}\n\\end{align}\\] All of those operators are self-adjoint with respect to the (real) inner product, i.e.\n\\[\\mathop{\\mathrm{RTr}}\\left[A^\\dagger \\mathcal{P}(B)\\right] = \\mathop{\\mathrm{RTr}}\\left[\\mathcal{P}(A)^\\dagger B\\right]\\]\nThis will be important to derive the reverse rules, and it shows that the cotangents will be automatically projected onto the subspace where the tangents are defined.\nClosely related to this is the remark that we made above about the dagger. We can define a dagger superoperator \\(\\mathcal{D}:{\\mathbb{C}}^{m \\times n} \\to {\\mathbb{C}}^{n\\times m}: A\\mapsto \\mathcal{D}({\\hat{A}})=A^\\dagger\\). Note that this is not a single (super)operator, but one for every matrix size, and in case of rectangular matrices, it maps to matrices of opposite size. It is a complex antilinear operator, but with respect to the real vector space structure, it is real linear and we can write, for all \\(A\\in {\\mathbb{C}}^{m \\times n}\\) and \\(B\\in {\\mathbb{C}}^{n \\times m}\\),\n\\[\\mathop{\\mathrm{RTr}}(B^\\dagger \\mathcal{D}(A))= \\mathop{\\mathrm{RTr}}(B^\\dagger A^\\dagger) = \\mathop{\\mathrm{RTr}}(BA) = \\mathop{\\mathrm{RTr}}(\\mathcal{D}(B)^\\dagger A).\\]\nWe can then define two more projection superoperators on the spaces of square matrices \\(A\\in {\\mathbb{C}}^{n\\times n}\\), namely those that project onto the hermitian and antihermitian part as \\[\\begin{align}\n\\mathcal{P}_{H}(A) &= \\frac{\\mathcal{I}+\\mathcal{D}}{2}(A) = \\frac{A+A^\\dagger}{2}\\\\\n\\mathcal{P}_{A}(A) &= \\frac{\\mathcal{I}-\\mathcal{D}}{2}(A) = \\frac{A-A^\\dagger}{2}\n\\end{align}\\] which also satisfy\n\\[\\mathop{\\mathrm{RTr}}(B^\\dagger \\mathcal{P}_{H/A}(A)) = \\mathop{\\mathrm{RTr}}(\\mathcal{P}_{H/A}(B)^\\dagger A)\\]\n\n\nUnitary matrices\nWe now turn to matrix constraints which are nonlinear, and give rise to known manifolds of special matrices. The first relevant case is a unitarity constraint, which requires \\(m=n\\): \\[U^\\dagger U = I_n = U^\\dagger U\\]\nFor the tangent directions, we find in this case that \\[U^\\dagger \\dot{U} + \\dot{U}^\\dagger U=O.\\]\nFor the tangent directions, the constraint has thus become linear and can easily be satisfied by parameterising \\(\\dot{U} = U\\dot{K}\\) where \\(\\dot{K}=-\\dot{K}^\\dagger\\), i.e. \\(\\dot{K}\\) is an antihermitian matrix, parameterised by \\(n(n-1)/2\\) complex parameters below the diagonal (and their conjugates above the diagonal) and \\(n\\) purely imaginary parameters on the diagonal.\n\n\nIsometric matrices\nIsometric matrices are rectangular matrices with \\(m \\geq n\\) that satisfy\n\\[Q^\\dagger Q = I_n\\]\nIn this case, \\(P=QQ^\\dagger\\) is not the identity, but an orthogonal projector (\\(P^2 =P=P^\\dagger\\))\nWe can think of \\(Q\\) as the first \\(n\\) column of a unitary \\((m \\times m)\\) matrix\n\\[U = \\begin{bmatrix} Q & Q_\\perp \\end{bmatrix}\\]\nwhere \\(Q_\\perp\\) are an additional \\((m-n)\\) orthonormal columns that complete the unitary matrix, and thus satisfy \\(Q^\\dagger Q_\\perp = O\\) and \\(Q_\\perp^\\dagger Q_\\perp = I_{m-n}\\). It furthermore holds that \\(Q_\\perp Q_\\perp = I_m - QQ^\\dagger\\).\nIf we parameterise the tangent of \\(U\\) as \\[\\begin{align}\n\\begin{bmatrix} \\dot{Q} & \\dot{Q}_\\perp\\end{bmatrix} = \\begin{bmatrix} Q & Q_\\perp\\end{bmatrix}\\begin{bmatrix} \\dot{K} & -\\dot{K}_\\perp^\\dagger \\\\ \\dot{K}_\\perp & \\dot{K}_{\\perp\\perp}\\end{bmatrix}\n\\end{align}\\] with \\(\\dot{K}=-\\dot{K}^\\dagger\\) (and also \\(\\dot{K}_{\\perp\\perp}=-\\dot{K}_{\\perp\\perp}^\\dagger\\)), then we find \\[\\begin{align}\n\\dot{Q} = Q \\dot{K} + Q_{\\perp} \\dot{K}_\\perp = Q \\dot{K} + \\dot{L}\n\\end{align}\\] where \\(\\dot{L}\\) only needs to satisfy \\(Q^\\dagger \\dot{L}=O\\). In practice, both representations of \\(\\dot{Q}\\) can be useful, depending on the situation:\n\nIf we have \\(n \\approx m\\) (e.g. \\(n=m/2\\)), it can be useful to explicitly determine a \\(m \\times (m-n)\\) matrix \\(Q_\\perp\\) that completes \\(Q\\) to unitary matrix, and to use the parameterisation \\(\\dot{Q} = Q \\dot{K} + Q_{\\perp} \\dot{K}_\\perp\\) where the \\((m-n) \\times n\\) matrix \\(\\dot{K}_\\perp\\) is completely unconstrained.\nOn the other hand, if \\(n \\ll m\\), then it might be that it is undesirable or even infeasible to compute and manipulate a matrix \\(Q_\\perp\\) of size \\(m \\times (m-n)\\). In that case, it is more efficient to use the parameterisation \\(Q \\dot{K} + \\dot{L}\\), where \\(\\dot{L}\\) also only has size \\((m\\times n)\\) (like \\(Q\\) and \\(\\dot{Q}\\)), but does need to satisfy the requirement that \\(Q^\\dagger \\dot{L} = 0\\). Even though we can exploit this condition on \\(\\dot{L}\\) in order to derive or simplify the equations that it needs to satisfy, we have to make sure that the final equation that determines \\(\\dot{L}\\) (which will always be a linear equation), is such that it either explicitly imposes the condition \\(Q^\\dagger \\dot{L} = 0\\), or is solved in such a way that the solution does satisfy this condition. Failure of doing so will typically result in the linear system for \\(\\dot{L}\\) having a nontrivial kernel and thus being singular."
  },
  {
    "objectID": "main.html#linear-problems-with-matrices",
    "href": "main.html#linear-problems-with-matrices",
    "title": "Automatic differentation rules for linear algebra",
    "section": "Linear problems with matrices",
    "text": "Linear problems with matrices\nAnother common operation with matrices that we encounter is that the matrix of interest, for example, the tangent \\(\\dot{A}\\) of some matrix, is only specified implicitly as the solution of a linear system. Indeed, the matrix factorisations are written down as equations for the final solution (rather than as the algorithm that gave rise to them). Differentating such defining equations gives rise to (sometimes coupled) linear equations that need to be solved.\n\nSylvester equation and Hadamard product\nThe most common type of linear system for a matrix quantity \\(X\\) that we encounter takes the form of a Sylvester equation\n\\[ A X - X B = C\\]\nwhere thus \\(X\\) is the unknown. Let us denote the left hand side as the (Sylvester) superoperator \\(\\mathcal{S}_{A,B}(X) = AX - XB\\). For \\(X\\in{\\mathbb{C}}^{m \\times n}\\), it is assumed that \\(A\\in{\\mathbb{C}}^{m\\times m}\\) and \\(B \\in {\\mathbb{C}}^{n \\times n}\\), so that \\(\\mathcal{S}_{A,B}\\) is indeed a linear (super)opeator \\({\\mathbb{C}}^{m\\times n} \\mapsto {\\mathbb{C}}^{m\\times n}\\), i.e. it is mapping matrices to the same size.\nThe solution of the linear system is then given by\n\\[X = S_{A,B}^{-1}(C)\\]\nprovided the inverse of the Sylvester superoperator exists. The Sylvester superoperator has a nontrivial kernel (and is thus not invertible) whenever \\(A\\) and \\(B\\) have a common eigenvalue \\(\\lambda\\). Indeed, with \\(A\\boldsymbol{v}=\\lambda \\boldsymbol{v}\\) and \\(\\boldsymbol{w}^\\dagger B = \\lambda \\boldsymbol{w}^\\dagger\\), it is clear that \\(\\mathcal{S}_{A,B}(\\boldsymbol{v}\\boldsymbol{w}^\\dagger) = O\\).\nIf both \\(A\\) and \\(B\\) can be diagonalised as \\(A=V_AD_AV_{A}^{-1}\\) and \\(B=V_BD_BV_{B}^{-1}\\), a simple way to obtain the solution of the Sylvester equation is as \\[\\begin{align}\nD_{A} \\tilde{X} - \\tilde{X}D_{B} = \\tilde{C}\n\\end{align}\\] with \\(\\tilde{X} = V_{A}^{-1}XV_{B}\\) and analoguously for \\(\\tilde{C}\\). In this case, the \\(i,j\\) component of this equation reads\n\\[(\\lambda_i - \\mu_j) \\tilde{X}_{ij} = \\tilde{C}_{ij}\\]\nwith \\((D_A)_{ii} = \\lambda_i\\) and \\((D_{B})_{jj}=\\mu_j\\) the respective eigenvalues. Here too, the importance of not having coinciding eigenvalues of \\(A\\) and \\(B\\) is clear, unless then also \\(\\tilde{C}_{ij} = 0\\). In the above form, we can solve the equation and find\n\\[ \\tilde{X}_{ij} = \\frac{\\tilde{C}_{ij}}{\\lambda_i - \\mu_j}\\]\nWe will typically denote this\n\\[\\tilde{X}=\\tilde{C} \\odot F\\]\nwhere \\(F_{ij} = \\frac{1}{\\lambda_i - \\mu_j}\\) and \\(\\odot\\) is the pointwise or Hadamard product, satisfying\n\\[(A\\odot B)_{ij} = (B\\odot A)_{ij} = A_{ij} B_{ij}\\].\nFinally, we investigate how these operations behave with respect to the real inner product. Note that the Sylvester superoperator acts as\n\\[\\begin{align}\n\\mathop{\\mathrm{RTr}}(X^\\dagger \\mathcal{S}_{A,B}(Y)) &= \\mathop{\\mathrm{RTr}}(X^\\dagger (AY-YB)) \\\\\n&= \\mathop{\\mathrm{RTr}}((A^\\dagger X - XB^\\dagger)^\\dagger Y)\\\\\n&= \\mathop{\\mathrm{RTr}}(\\mathcal{S}_{A^\\dagger,B^\\dagger}(X)^\\dagger Y)\n\\end{align}\\]\nAs a consequence, when we have a variable \\(\\dot{X} = \\mathcal{S}_{A,B}^{-1}(\\dot{Y})\\), it must hold that\n\\[\\begin{align}\n\\mathop{\\mathrm{RTr}}(\\breve{X}^\\dagger \\dot{X})&= \\mathop{\\mathrm{RTr}}(\\breve{X}^\\dagger \\mathcal{S}_{A,B}^{-1}(\\dot{Y}))\\\\\n&= \\mathop{\\mathrm{RTr}}(\\mathcal{S}_{A^\\dagger,B^\\dagger}^{-1}(\\breve{X})^\\dagger \\dot{Y})\n\\end{align}\\]\nand thus \\(\\breve{Y} = \\mathcal{S}_{A^\\dagger,B^\\dagger}^{-1}(\\breve{X})\\).\nRelated to this is the behaviour of the Hadamard product with respect to this inner product, for which we obtain\n\\[\\mathop{\\mathrm{RTr}}(A^\\dagger (B \\odot C)) = \\mathop{\\mathrm{RTr}}((A\\odot \\overline{B})^\\dagger C).\\]\nAll of these results will prove useful below."
  },
  {
    "objectID": "main.html#gauge-freedom",
    "href": "main.html#gauge-freedom",
    "title": "Automatic differentation rules for linear algebra",
    "section": "Gauge freedom",
    "text": "Gauge freedom\nA final issue that is in some sense “orthogonal” to constraints is that of gauge freedom. This arises when certain variables are not uniquely determined, such as the phase of eigenvectors or singular vectors. For example, it could happen that an output matrix \\(A\\) of a certain step of the computation is only determined up to an overall multiplication with another matrix \\(D\\) (for example diagonal, or unitary, …), so that we could also have obtained \\(A' = AD\\).\nIf we now consider a one-parameter family of such equivalent matrices, this gives rise to tangent vectors \\(\\dot{A}=A\\dot{D}\\). However, for objective functions \\(f\\) that are insensitive to such gauge changes, it should hold that the partial derivatives \\(\\breve{A} = \\frac{\\partial f}{\\partial \\overline{A}_{ij}}\\) should be such that\n\\[\\mathop{\\mathrm{RTr}}(\\breve{A}^\\dagger A\\dot{D}) = 0\\]\nfor all \\(\\dot{D}\\) that are allowed. Such a condition can easily be checked at the beginning of a reverse rule, and then warned about if it is not satisfied, as this would be indicative of an objective function that uses \\(A\\) in a way that depends on implementation details, i.e. an objective function that is not “gauge invariant”."
  },
  {
    "objectID": "main.html#full-rank-case",
    "href": "main.html#full-rank-case",
    "title": "Automatic differentation rules for linear algebra",
    "section": "Full rank case",
    "text": "Full rank case\nConsider a rectangular matrix \\(A\\in {\\mathbb{C}}^{m \\times n}\\). The typical scenario is when \\(m \\geq n\\) and \\(\\mathop{\\mathrm{rank}}(A)=n\\), i.e. \\(A\\) has full rank. In this case, there exists a decomposition\n\\[A  =Q R\\]\nwith \\(Q\\in {\\mathbb{C}}^{m\\times n}\\) an isometric matrix (\\(Q^\\dagger Q=I_n\\)) and \\(R\\in{\\mathbb{C}}^{n \\times n}\\) an upper-triangular matrix (\\(R_{k,l}=0\\) for \\(k &gt; l\\)). This decomposition can be made unique by choosing the diagonal elemenents \\(R_{kk} &gt; 0\\). For \\(\\mathop{\\mathrm{rank}}(A)=\\mathop{\\mathrm{rank}}(R)=n\\), none of those elements can be zero and \\(R\\) is invertible, with its inverse \\(R^{-1}\\) also being an upper triangular matrix\nFor the forward rule, we find\n\\[\\dot{A} = \\dot{Q} R + Q \\dot{R}\\]\nwhere we now insert \\(\\dot{Q} = Q\\dot{K}+\\dot{L}\\) as described above. Projecting those equations onto the column space of \\(Q\\) and onto the orthogonal complement thereof, we obtain\n\\[\\begin{align}\nQ^\\dagger \\dot{A} R^{-1} &= \\dot{K}  + \\dot{R}R^{-1}\\\\\n(I- QQ^\\dagger)\\dot{A}R^{-1} &= \\dot{L}\n\\end{align}\\]\nusing \\((I- QQ^\\dagger)\\dot{L} =\\dot{L}\\) by definition. For the first equation, the second term in the right hand side is also upper triangular with a real diagonal, whereas the first term is antihermitian. The first term is thus determined by the part below the diagonal on the left hand side, as well as the imaginary part of its diagonal. We can then write the forward rule as\n\\[\\begin{align}\n\\dot{K} &= \\mathcal{P}_L(Q^\\dagger \\dot{A} R^{-1}) + \\mathcal{P}_{iD}(Q^\\dagger \\dot{A} R^{-1}) - \\mathcal{P}_L(Q^\\dagger \\dot{A} R^{-1})^\\dagger\\\\\n\\dot{R} &= \\left[\\mathcal{P}_{rD}(Q^\\dagger \\dot{A} R^{-1}) + \\mathcal{P}_U(Q^\\dagger \\dot{A} R^{-1}) +  \\mathcal{P}_L(Q^\\dagger \\dot{A} R^{-1})^\\dagger \\right]R\\\\\n\\dot{L} &= (I- QQ^\\dagger)\\dot{A}R^{-1}\\\\\n\\Rightarrow \\dot{Q} &= Q \\dot{K} + \\dot{L} \\\\\n&= \\dot{A}R^{-1} + Q\\left[-Q^\\dagger \\dot{A}R^{-1} + \\mathcal{P}_L(Q^\\dagger \\dot{A} R^{-1})+ \\mathcal{P}_{iD}(Q^\\dagger \\dot{A} R^{-1}) - \\mathcal{P}_L(Q^\\dagger \\dot{A} R^{-1})^\\dagger\\right]\\\\\n&= \\dot{A}R^{-1} - Q\\left[\\mathcal{P}_U(Q^\\dagger \\dot{A} R^{-1}) + \\mathcal{P}_{rD}(Q^\\dagger \\dot{A} R^{-1}) + \\mathcal{P}_L(Q^\\dagger \\dot{A} R^{-1})^\\dagger\\right] = \\dot{A}R^{-1} - Q\\dot{R}R^{-1}\n\\end{align}\\]\nTo derive the reverse rule, we then start from \\(\\mathop{\\mathrm{RTr}}(\\breve{Q}^\\dagger\\dot{Q}) + \\mathop{\\mathrm{RTr}}(\\breve{R}^\\dagger\\dot{R})\\) and insert the expressions above, which we then simplify by making use of the cyclic invariance of the trace and the self-adjointness of the \\(\\mathcal{P}\\) operators with respect to the real \\(\\mathop{\\mathrm{RTr}}\\) inner product. We find\n\\[\\begin{align}\n\\mathop{\\mathrm{RTr}}(\\breve{Q}^\\dagger\\dot{Q}) &+ \\mathop{\\mathrm{RTr}}(\\breve{R}^\\dagger\\dot{R}) \\\\\n&= \\mathop{\\mathrm{RTr}}(\\breve{Q}^\\dagger \\dot{A}R^{-1}) + \\mathop{\\mathrm{RTr}}([\\breve{R}^\\dagger - R^{-1} \\breve{Q}^\\dagger Q]\\dot{R})\\\\\n&= \\mathop{\\mathrm{RTr}}([\\breve{Q}R^{-\\dagger}]^\\dagger \\dot{A}) + \\mathop{\\mathrm{RTr}}([\\breve{R} -  Q^\\dagger\\breve{Q}R^{-\\dagger}]^\\dagger\\dot{R})\\\\\n&= \\mathop{\\mathrm{RTr}}([\\breve{Q}R^{-\\dagger}]^\\dagger \\dot{A}) + \\mathop{\\mathrm{RTr}}([\\breve{R} -  Q^\\dagger\\breve{Q}R^{-\\dagger}]^\\dagger [\\mathcal{P}_{rD}(Q^\\dagger \\dot{A} R^{-1}) + \\mathcal{P}_U(Q^\\dagger \\dot{A} R^{-1}) +  \\mathcal{P}_L(Q^\\dagger \\dot{A} R^{-1})^\\dagger ]R)\\\\\n&= \\mathop{\\mathrm{RTr}}([\\breve{Q}R^{-\\dagger}]^\\dagger \\dot{A}) + \\mathop{\\mathrm{RTr}}([\\breve{R}R^\\dagger -  Q^\\dagger\\breve{Q}]^\\dagger [\\mathcal{P}_{rD}(Q^\\dagger \\dot{A} R^{-1}) + \\mathcal{P}_U(Q^\\dagger \\dot{A} R^{-1}) +  \\mathcal{P}_L(Q^\\dagger \\dot{A} R^{-1})^\\dagger ])\\\\\n&= \\mathop{\\mathrm{RTr}}([\\breve{Q}R^{-\\dagger}]^\\dagger \\dot{A}) + \\\\\n&\\qquad \\mathop{\\mathrm{RTr}}([ \\mathcal{P}_{rD}(\\breve{R}R^\\dagger -  Q^\\dagger\\breve{Q})+\\mathcal{P}_{U}(\\breve{R}R^\\dagger -  Q^\\dagger\\breve{Q})+\\mathcal{P}_{L}((\\breve{R}R^\\dagger -  Q^\\dagger\\breve{Q})^\\dagger)]^\\dagger Q^\\dagger \\dot{A} R^{-1})\n\\end{align}\\]\nNote that, for the last term in the square brackets, we first had to use \\(\\mathop{\\mathrm{RTr}}(A^\\dagger B^\\dagger) = \\mathop{\\mathrm{RTr}}(AB) = \\mathop{\\mathrm{RTr}}((A^\\dagger)^\\dagger B)\\), before we could use the self-adjointness property of \\(\\mathcal{P}_L\\). Halfway in the computation, we have also introduced the short-hand notation \\(R^{-\\dagger} = (R^{-1})^\\dagger = (R^\\dagger)^{-1}\\). If we furthermore use that \\(\\mathcal{P}_L(A^\\dagger) = \\mathcal{P}_U(A)^\\dagger\\), we can further rewrite this result as\n\\[\\begin{align}\n\\mathop{\\mathrm{RTr}}(\\breve{Q}^\\dagger\\dot{Q}) &+ \\mathop{\\mathrm{RTr}}(\\breve{R}^\\dagger\\dot{R}) \\\\\n&= \\mathop{\\mathrm{RTr}}([\\breve{Q}R^{-\\dagger}]^\\dagger \\dot{A}) + \\\\\n&\\qquad \\mathop{\\mathrm{RTr}}([ \\mathcal{P}_{rD}(\\breve{R}R^\\dagger -  Q^\\dagger\\breve{Q})+\\mathcal{P}_{U}(\\breve{R}R^\\dagger -  Q^\\dagger\\breve{Q})+\\mathcal{P}_{U}(\\breve{R}R^\\dagger -  Q^\\dagger\\breve{Q})^\\dagger]^\\dagger Q^\\dagger \\dot{A} R^{-1})\\\\\n&=  \\mathop{\\mathrm{RTr}}([\\breve{Q}R^{-\\dagger} + Q(\\mathcal{P}_{rD}(\\breve{R}R^\\dagger -  Q^\\dagger\\breve{Q})+\\mathcal{P}_{U}(\\breve{R}R^\\dagger -  Q^\\dagger\\breve{Q})+\\mathcal{P}_{U}(\\breve{R}R^\\dagger -  Q^\\dagger\\breve{Q})^\\dagger)R^{-\\dagger}]^\\dagger \\dot{A})\n\\end{align}\\]\nfrom which we obtain the final result for the reverse rule\n\\[\\begin{align}\n\\breve{A} = \\breve{Q}R^{-\\dagger} + Q(\\mathcal{P}_{rD}(\\breve{R}R^\\dagger -  Q^\\dagger\\breve{Q})+\\mathcal{P}_{U}(\\breve{R}R^\\dagger -  Q^\\dagger\\breve{Q})+\\mathcal{P}_{U}(\\breve{R}R^\\dagger -  Q^\\dagger\\breve{Q})^\\dagger)R^{-\\dagger}\n\\end{align}\\]"
  },
  {
    "objectID": "main.html#general-result",
    "href": "main.html#general-result",
    "title": "Automatic differentation rules for linear algebra",
    "section": "General result",
    "text": "General result\nNow suppose that we have a matrix \\(A\\in{\\mathbb{C}}^{m \\times n}\\), where we do not require that \\(m \\geq n\\), and furthermore can have \\(\\mathop{\\mathrm{rank}}(A) = r \\leq \\min(m,n)\\). Let us split \\(m=m_1 + m_2 + m_3\\) and \\(n=n_1 + n_2 =\\) with \\(n_1 = m_1 = r\\), \\(m_2 = \\min(m,n)-r\\), \\(n_2 = n-r\\) and \\(m_3=m-\\min(m,n)\\). We still have to make one assumption, which is that \\(A\\) is such that its first \\(n_1=r\\) columns already span its image (column space), so that a full rank QR decomposition of size \\(m \\times r\\) could be obtained from these columns.\nWe now write the QR decomposition using a square (and thus unitary) matrix \\(Q\\), in the form\n\\[\\begin{bmatrix} A_1 & A_2\\end{bmatrix} = \\begin{bmatrix} Q_1 & Q_2 &  Q_3\\end{bmatrix} \\begin{bmatrix} R_{11} & R_{12} \\\\ O& O\\\\  O& O\\end{bmatrix}\\]\nHere, \\(A_j \\in {\\mathbb{C}}^{m \\times n_j}\\), \\(Q_j \\in {\\mathbb{C}}^{m \\times m_i}\\) and \\(R_{ij} \\in {\\mathbb{C}}^{m_i \\times n_j}\\) for \\(i=1,2,3\\) and \\(j=1,2\\). Note that \\(R_{21}\\), \\(R_{31}\\) and \\(R_{32}\\) are always zero because of the upper triangularity of \\(R\\). A compact QR decomposition would not return \\(Q_3\\) and the bottom row of \\(R\\), since already \\(m_1 + m_2 = \\min(m,n)\\). We include it here for completeness.\nNote that the partitioning of \\(m\\) and \\(n\\) is dependent on the current matrix \\(A\\), and we only have \\(m_2&gt;0\\) and \\(n_2&gt;0\\) when both \\(m\\) and \\(n\\) exceed the rank \\(r\\) of the current matrix. That the block \\(R_{22}\\) is zero expresses precisely this rank deficiency, but it will become nonzero as soon as we perturb away from it. A consequence of this is that \\(\\dot{R}_{22}\\) will be nonzero. To assess the effect of \\(R_{22}=0\\), we will actually replace it with \\(R_{22} = \\epsilon S_{22}\\) and study the limit \\(\\epsilon \\to 0\\).\nNote that \\(Q_3\\) (present for \\(m &gt; n\\)) can never be uniquely defined, whereas \\(Q_2\\) becomes ambiguous in the limit \\(\\epsilon \\to 0\\) and will change abruptly under small variations. Hence, we can expect that \\(\\dot{Q}_{22}\\) will scale as \\(\\frac{1}{\\epsilon}\\). We first try to derive a forward rule by writing\n\\[\\begin{bmatrix} \\dot{A}_1 & \\dot{A}_2\\end{bmatrix} = \\begin{bmatrix} \\dot{Q}_1 & \\dot{Q}_2 & \\dot{Q}_3 \\end{bmatrix} \\begin{bmatrix} R_{11} & R_{12} \\\\ O& \\epsilon S_{22}\\\\ O& O\\end{bmatrix} + \\begin{bmatrix} Q_1 & Q_2 & Q_3\\end{bmatrix} \\begin{bmatrix} \\dot{R}_{11} & \\dot{R}_{12} \\\\ O& \\dot{R}_{22} \\\\ O& O\\end{bmatrix}\\]\nwhich gives rise to\n\\[\\begin{align}\n\\dot{A}_1 &= \\dot{Q}_1 R_{11} + Q_1 \\dot{R}_{11}\\\\\n\\dot{A}_2 &= \\dot{Q}_1 R_{12} + \\epsilon \\dot{Q}_2 S_{22} + Q_1 \\dot{R}_{12} + Q_2 \\dot{R}_{22}\n\\end{align}\\]\nThe first equation fixes \\(\\dot{Q}_1\\) and \\(\\dot{R}_{11}\\) completely, using the forward rule of the full rank QR derived above. Suppose that the resulting \\(\\dot{Q}_1\\) takes the form \\(\\dot{Q}_1 = Q_1 \\dot{K}_{11} + \\dot{L}_1\\). From the structure of \\(\\dot{Q}\\), we know that \\(\\dot{L}_1 = Q_2 \\dot{K}_{21} + Q_3 \\dot{K}_{31}\\), and then \\(\\dot{Q}_2 = -Q_1 \\dot{K}_{21}^\\dagger + Q_2 \\dot{K}_{22}+Q_3 \\dot{K}_{32}\\), where thus \\(\\dot{K}_{21} = Q_2^\\dagger \\dot{L}_1 =  Q_2^\\dagger \\dot{Q}_1\\) and \\(\\dot{K}_{22}\\) and \\(\\dot{K}_{32}\\) are new variables (with \\(\\dot{K}_{22}\\) antihermitian). Inserting this expression for \\(\\dot{Q}_2\\) into the second equation and projection onto \\(Q_1\\), \\(Q_2\\) and \\(Q_3\\) separately, we obtain\n\\[\\begin{align}\nQ_1^\\dagger (\\dot{A}_2-\\dot{Q}_1 R_{12}) &= \\epsilon Q_1^\\dagger\\dot{Q}_2 S_{22} + \\dot{R}_{12} = -\\epsilon \\dot{Q}_1^\\dagger Q_2 S_{22} + \\dot{R}_{12}\\\\\nQ_2^\\dagger (\\dot{A}_2-\\dot{Q}_1 R_{12}) &= \\epsilon \\dot{K}_{22} S_{22} + \\dot{R}_{22} \\\\\nQ_3^\\dagger (\\dot{A}_2-\\dot{Q}_1 R_{12}) &= \\epsilon \\dot{K}_{32} S_{22}\n\\end{align}\\]\nWe see that the divergent contribution in \\(\\dot{Q}_2\\) disappears along the component \\(Q_1^\\dagger\\dot{Q}_2\\), because this equals \\(-Q_2^\\dagger\\dot{Q}_1\\). Here, we can safely take the limit \\(\\epsilon \\to 0\\), which yields\n\\[\\begin{align}\n\\dot{R}_{12} = Q_1^\\dagger (\\dot{A}_2-\\dot{Q}_1 R_{12}).\n\\end{align}\\]\nThe divergent contributions will thus appear in \\(\\dot{K}_{22}=Q_2 \\dot{Q}_2\\) and \\(\\dot{K}_{32}=Q_3 \\dot{Q}_2\\). As the original matrix \\(A\\) only fixes \\(Q_1\\), but not \\(Q_2\\) and \\(Q_3\\) (except for the fact that together they span the orthogonal complement of \\(Q_1\\)), it follows naturally that the objective function is such that \\(Q_2^\\dagger \\breve{Q}_2\\) and \\(Q_3^\\dagger \\breve{Q}_3\\) are approximately zero. The fact that we know that \\(Q_2\\) needs to be orthogonal to \\(Q_1\\) does give meaning tot the projection \\(Q_1^\\dagger \\breve{Q}_2\\), which thus does not need to be zero. As \\(Q_3\\) is typically not part of the return result, we do not consider its tangent and cotangent at all.\nPS: In practice, this doesn’t work. For any small update \\(A \\to A +\\epsilon \\dot{A}\\), we know that the change to \\(Q_2\\) will be of order 1, i.e. writing this is \\(Q_2 + \\epsilon \\dot{Q}_2\\), this implies that \\(\\dot{Q}_2\\) is \\(\\mathop{\\mathrm{\\mathscr{O}}}(\\epsilon^{-1})\\). While it is true that it is only \\(Q_2^\\dagger \\dot{Q}_2\\) that order \\(\\epsilon^{-1}\\), and \\(Q_1^\\dagger \\dot{Q}_2\\) is indeed of order 1, it does not correspond to \\(-(Q_2^\\dagger \\dot{Q}_1)^\\dagger\\) up to order \\(\\epsilon\\), because there are order \\(\\epsilon * \\epsilon^{-1} = 1\\) corrections. As a consequence, the physically meaningfull contribution to \\(Q_1^\\dagger \\breve{Q}_2\\) is polluted and it only makes sense to check for \\(\\breve{Q}_2\\) as a whole to be zero as a condition for a gauge-invariant cost function.\nWhat remains to be discussed is the faith of \\(\\dot{R}_{22}\\), and thus, whether there can be a meaningfull cotangent \\(\\breve{R}_{22}\\) associated to it. As it seems from these equations, it couples to \\(Q_2\\), which is not well defined, which seems to imply that also \\(\\dot{R}_{22}\\) can impossibly defined unambiguously and would potentially also be divergent. It turns out that the truth is more subtle. Consider thereto\n\\[\\begin{align}\n(I-Q_1Q_1^\\dagger)(\\dot{A}_2 - \\dot{Q}_1 R_{12})=\\dot{A}_2 - \\dot{Q}_1 R_{12} - Q_1\\dot{R}_{12} = \\epsilon \\dot{Q}_2 S_22 + Q_2 \\dot{R}_{22}.\n\\end{align}\\]\nThe left hand side is by construction orthogonal to \\(Q_1\\), and thus supported in its orthogonal complement. If it would happen that the QR decomposition of the left hand side is exactly \\(Q_2 \\dot{R}_{22}\\), there would be no need for divergent contributions in \\(\\dot{Q}_{22}\\), i.e. we would have \\(\\dot{K}_{22} =O\\) and \\(\\dot{K}_{33}=O\\). Note also that, as \\(\\dot{R}_{22}\\) is a tangent to \\(R_{22}=O\\), we want to diagonal elements of \\(\\dot{R}_{22}\\) to be positive instead of just real, if the whole decomposition is to represent a QR decomposition with positive diagonal elements. Any deviation between the value of \\(Q_2\\) that happens to be chosen in the QR decomposition of \\(A\\), and the Q-factor in the QR decomposition of \\(\\dot{A}_2 - \\dot{Q}_1 R_{12} - Q_1\\dot{R}_{12}\\) gives rise to divergent contributions in \\(\\dot{Q}_2\\), but does not affect \\(\\dot{R}_{22}\\). Hence, \\(\\dot{R}_{22}\\) is well defined and nondiverging as the R-factor in the (positive) QR decomposition of \\(\\dot{A}_2 - \\dot{Q}_1 R_{12} - Q_1\\dot{R}_{12}\\). However, while the R-factor in a QR decomposition is homogeneous (rescaling the matrix rescales the R-factor) it is not additive (the R-factor of the sum of two matrices is not the sum). Hence, \\(\\dot{R}_{22}\\), while being non-diverging, does not satisfy the linearity properties associated with a well defined tangent. Hence, we must also assume (or verify) that the associated cotangent \\(\\breve{R}_{22}\\) approximates zero for a gauge invariant cost function.\nFor the reverse rule, we now start from \\[\\begin{align}\n\\mathop{\\mathrm{RTr}}(\\breve{Q}_1^\\dagger\\dot{Q}_1) &+ \\mathop{\\mathrm{RTr}}(\\breve{Q}_2^\\dagger\\dot{Q}_2)+ \\mathop{\\mathrm{RTr}}(\\breve{R}_{11}^\\dagger\\dot{R}_{11})+ \\mathop{\\mathrm{RTr}}(\\breve{R}_{12}^\\dagger\\dot{R}_{12})+ \\mathop{\\mathrm{RTr}}(\\breve{R}_{22}^\\dagger\\dot{R}_{22})\\end{align}\\]\nand use our assumptions that \\(\\breve{R}_{22}=0\\) and \\(\\breve{Q}_2 = (Q_1Q_1^\\dagger + Q_2Q_2^\\dagger + Q_3Q_3^\\dagger)\\breve{Q}_2 = Q_1Q_1^\\dagger\\breve{Q}_2\\) together with \\(Q_1^\\dagger\\dot{Q}_2 = -\\dot{Q}_1^\\dagger Q_2\\) to write\n\\[\\begin{align}\n\\mathop{\\mathrm{RTr}}(\\breve{Q}_1^\\dagger\\dot{Q}_1) &- \\mathop{\\mathrm{RTr}}((Q_1^\\dagger \\breve{Q}_2]^\\dagger\\dot{Q}_1^\\dagger Q_2)+ \\mathop{\\mathrm{RTr}}(\\breve{R}_{11}^\\dagger\\dot{R}_{11})+ \\mathop{\\mathrm{RTr}}(\\breve{R}_{12}^\\dagger\\dot{R}_{12}) \\\\\n&= \\mathop{\\mathrm{RTr}}([\\breve{Q}_1-Q_2\\breve{Q}_2^\\dagger Q_1]^\\dagger\\dot{Q}_1) + \\mathop{\\mathrm{RTr}}(\\breve{R}_{11}^\\dagger\\dot{R}_{11})+ \\mathop{\\mathrm{RTr}}(\\breve{R}_{12}^\\dagger Q_1^\\dagger(\\dot{A}_2 - \\dot{Q}_1R_{12}))\\\\\n&= \\mathop{\\mathrm{RTr}}([\\breve{Q}_1-Q_2\\breve{Q}_2^\\dagger Q_1 - Q_1\\breve{R}_{12}R_{12}^\\dagger]^\\dagger\\dot{Q}_1) + \\mathop{\\mathrm{RTr}}(\\breve{R}_{11}^\\dagger\\dot{R}_{11})+ \\mathop{\\mathrm{RTr}}([Q_1\\breve{R}_{12}]^\\dagger \\dot{A}_2)\n\\end{align}\\]\nFrom this point on, we can follow the derivation of the full rank case to reduce \\(\\dot{Q}_1\\) and \\(\\dot{R}_11\\) to \\(\\dot{A}_1\\), which then defines the cotangent \\(\\breve{A}_1\\), while we can immediately read of \\(\\breve{A}_2\\). We thus find\n\\[\\begin{align}\n\\breve{A}_1 &= \\breve{Q}R_{11}^{-\\dagger} + Q_1(\\mathcal{P}_{rD}(\\breve{R}_{11}R_{11}^\\dagger -  Q_1^\\dagger\\breve{Q})+\\mathcal{P}_{U}(\\breve{R}_{11}R_{11}^\\dagger -  Q_1^\\dagger\\breve{Q})+\\mathcal{P}_{U}(\\breve{R}_{11}R_{11}^\\dagger -  Q_1^\\dagger\\breve{Q})^\\dagger)R_{11}^{-\\dagger}\\\\\n\\breve{A}_2 &= Q_1\\breve{R}_{12}\n\\end{align}\\] where the value \\[\\begin{align}\n\\breve{Q} = \\breve{Q}_1-Q_2\\breve{Q}_2^\\dagger Q_1 - Q_1\\breve{R}_{12}R_{12}^\\dagger\n\\end{align}\\] should be inserted."
  },
  {
    "objectID": "main.html#hermitian-eigenvalue-decomposition",
    "href": "main.html#hermitian-eigenvalue-decomposition",
    "title": "Automatic differentation rules for linear algebra",
    "section": "Hermitian eigenvalue decomposition",
    "text": "Hermitian eigenvalue decomposition\nGiven a Hermitian matrix \\(A=A^\\dagger\\) with \\(A\\in{\\mathbb{C}}^{n\\times n}\\), it admits an eigenvalue decomposition of the form\n\\[A = U D U^{\\dagger}\\]\nwith \\(U\\in{\\mathbb{C}}^{n\\times n}\\) unitary and \\(D\\) a real diagonal matrix containing the eigenvalues. This decomposition is not unique, as we can multiply \\(U\\) with another unitary matrix \\(V\\) that commutes with \\(D\\). If all eigenvalues in \\(D\\) are mutually distinct, this amounts to \\(V\\) being diagonal and unitary, or thus, a matrix with pure complex phases. If some eigenvalues are repeated, \\(V\\) can take a block diagonal form.\nLet us first derive the forward rule from writing the decomposition as \\(AU =UD\\)\n\\[\\dot{A} U + A \\dot{U} = \\dot{U} D + U\\dot{D}\\]\nand inserting \\(\\dot{U}=U\\dot{K}\\) with \\(K=-K^\\dagger\\), we obtain\n\\[U^\\dagger \\dot{A}U= \\dot{K}D - D\\dot{K} + \\dot{D}\\]\nor, in components, where we denote the \\(i\\)th column of \\(U\\) as \\(\\boldsymbol{u}_i\\), the \\(i\\)th eigenvector, and the \\(i\\)th diagonal element of \\(D\\) as \\(\\lambda_i\\), the corresponding eigenvalue:\n\\[\\boldsymbol{u}_i^\\dagger \\dot{A} \\boldsymbol{u}_j = \\dot{K}_{ij} (\\lambda_j - \\lambda_i) + \\dot{\\lambda}_i \\delta_{ij}\\]\nNote that \\(\\dot{K}_{ij}\\) cannot be determined if \\(\\lambda_j = \\lambda_i\\), which includes in particular the diagonal elements \\(i=j\\). This corresponds exactly to the indeterminedness of \\(U\\) itself in these cases. Any adjoint variables originating from a “gauge-invariant” objective function should thus satisfy\n\\[\\mathop{\\mathrm{RTr}}(\\breve{U}^\\dagger U K) = 0\\]\nfor any antihermitian \\(K\\) that contains nonzero entries \\((i,j)\\) only where \\(\\lambda_i = \\lambda_j\\). This leads to\n\\[(U^\\dagger \\breve{U})_{ij} - \\overline{(U^\\dagger \\breve{U})_{ji}} = 0,\\quad \\text{for all $1\\leq i\\leq j\\leq n$ for which $\\lambda_i = \\lambda_j$}\\]\nWe can then solve the forward rule as\n\\[\\begin{align}\n\\dot{K}_{ij} &= \\begin{cases} 0,& i = j\\text{ or } \\lambda_i=\\lambda_j \\\\ \\frac{\\boldsymbol{u}_i^\\dagger \\dot{A}\\boldsymbol{u}_j}{\\lambda_i-\\lambda_j},&\\text{otherwise}\\end{cases}\\\\\n\\dot{\\lambda}_i &= \\boldsymbol{u}_i^\\dagger \\dot{A}\\boldsymbol{u}_i\n\\end{align}\\]\nThe rule for \\(\\dot{K}\\) is often written as \\(\\dot{K} = F \\odot (U^\\dagger \\dot{A}U)\\) with \\(\\odot\\) the Hadamard product (see above) and \\(F_{ij} = \\begin{cases} 0, &\\lambda_i = \\lambda_j\\\\ \\frac{1}{\\lambda_j-\\lambda_i},&\\text{otherwise}\\end{cases}\\).\nIt is furthermore important whether we consider \\(D\\), and thus also its adjoint \\(\\breve{D}\\), to be a matrix, or only a vector of its diagonal elements. Here, we make the former choice. Indeed, we can write the forward rules as\n\\[\\begin{align}\n\\dot{K} &= F \\odot (U^\\dagger \\dot{A}U)\\\\\n\\dot{D} &= \\mathcal{P}_{rD}(U^\\dagger \\dot{A}U)\n\\end{align}\\] with \\(\\mathcal{P}_{rD}\\) the superoperator from before, that projects away everything but the real part of the diagonal. Note that the diagonal elements are anyway real in this case. If the adjoint \\(\\breve{D}\\) would be a full matrix, all its non-diagonal elements as well as possible imaginary contributions on the diagonal are projected away, because of the self-adjointness property of this superoperator. Indeed, we can now obtain the backward rule as\n\\[\\begin{align}\n\\mathop{\\mathrm{RTr}}(\\breve{U}^\\dagger \\dot{U})+ \\mathop{\\mathrm{RTr}}(\\breve{D}^\\dagger \\dot{D}) &= \\mathop{\\mathrm{RTr}}(\\breve{U}^\\dagger U\\dot{K})+ \\mathop{\\mathrm{RTr}}(\\breve{D}^\\dagger \\dot{D})\\\\\n&= \\mathop{\\mathrm{RTr}}\\left[(U^\\dagger \\breve{U})^\\dagger (F \\odot (U^\\dagger\\dot{A}U))\\right] + \\mathop{\\mathrm{RTr}}\\left[\\breve{D}^\\dagger \\mathcal{P}_{rD}(U^\\dagger \\dot{A}U)\\right]\\\\\n&= \\mathop{\\mathrm{RTr}}\\left[(F\\odot(U^\\dagger \\breve{U}))^\\dagger (U^\\dagger\\dot{A}U)\\right] + \\mathop{\\mathrm{RTr}}\\left[\\mathcal{P}_{rD}(\\breve{D})^\\dagger (U^\\dagger \\dot{A}U)\\right]\\\\\n&=\\mathop{\\mathrm{RTr}}\\left[(U(F\\odot (U^\\dagger \\breve{U}) + \\mathcal{P}_{rD}(\\breve{D}))U^\\dagger)^\\dagger \\dot{A}\\right]\n\\end{align}\\]\nand thus obtain\n\\[\\breve{A} = U(F\\odot (U^\\dagger \\breve{U}) + \\mathcal{P}_{rD}(\\breve{D}))U^\\dagger\\]\nwith thus \\(F_{ij} = \\begin{cases} 0, &\\lambda_i = \\lambda_j\\\\ \\frac{1}{\\lambda_j-\\lambda_i},&\\text{otherwise}\\end{cases}\\). As \\(F\\) is real, no complex conjugations are introduced when it \\(F\\) moved from the tangent to the cotangent variables."
  },
  {
    "objectID": "main.html#nonhermitian-eigenvalue-decomposition",
    "href": "main.html#nonhermitian-eigenvalue-decomposition",
    "title": "Automatic differentation rules for linear algebra",
    "section": "Nonhermitian eigenvalue decomposition",
    "text": "Nonhermitian eigenvalue decomposition\nMost of this analysis can be repeated for the nonhermitian case, where the eigenvalue decomposition reads\n\\[A =VDV^{-1} \\iff AV=VD\\]\nwith \\(D\\) still diagonal, but \\(V\\) no longer unitary. Still parameterising \\(\\dot{V}=V\\dot{K}\\), where \\(\\dot{K}\\) can now be a general matrix, we find\n\\[\\begin{align}\n\\dot{K}&= F \\odot (V^{-1}\\dot{A}V)\\\\\n\\dot{D}&= \\mathcal{P}_D (V^{-1}\\dot{A}V)\n\\end{align}\\]\nwith \\(\\mathcal{P}_{D} = \\mathcal{P}_{rD}+\\mathcal{P}_{iD}\\) the superoperator that selects the diagonal entries (real + imaginary part) and removes all non-diagonal entries. The matrix \\(F\\) is still given by\n\\[F_{ij} = \\begin{cases} 0, &\\lambda_i = \\lambda_j\\\\ \\frac{1}{\\lambda_j-\\lambda_i},&\\text{otherwise}\\end{cases}\\]\nand still uses the fact that \\(\\dot{K}_{ij}\\) cannot be determined if \\(\\lambda_i = \\lambda_j\\), and that we just put these components equal to zero. The major difference with the Hermitian case is that now the eigenvalues \\(\\lambda_i = D_{ii}\\) can no longer assumed to be real.\nFrom this result, we obtain the reverse rule\n\\[\\begin{align}\n\\mathop{\\mathrm{RTr}}(\\breve{V}^\\dagger \\dot{V})+ \\mathop{\\mathrm{RTr}}(\\breve{D}^\\dagger \\dot{D})&=  \\mathop{\\mathrm{RTr}}(\\breve{V}^\\dagger V\\dot{K})+ \\mathop{\\mathrm{RTr}}(\\breve{D}^\\dagger \\dot{D})\\\\\n&= \\mathop{\\mathrm{RTr}}\\left[(V^\\dagger\\breve{V})^\\dagger (F \\odot (V^{-1}\\dot{A}V))\\right] + \\mathop{\\mathrm{RTr}}\\left[\\breve{D}^\\dagger \\mathcal{P}_{D}(V^{-1} \\dot{A}V)\\right]\\\\\n&= \\mathop{\\mathrm{RTr}}\\left[(\\overline{F}\\odot (V^\\dagger\\breve{V}))^\\dagger (V^{-1}\\dot{A}V)\\right] + \\mathop{\\mathrm{RTr}}\\left[\\mathcal{P}_{D}(\\breve{D})^\\dagger (V^{-1} \\dot{A}V)\\right]\\\\\n&=\\mathop{\\mathrm{RTr}}\\left[(V^{-\\dagger}(\\overline{F}\\odot (V^\\dagger\\breve{V}) + \\mathcal{P}_{D}(\\breve{D}))V^{\\dagger})^\\dagger \\dot{A}\\right]\n\\end{align}\\]\nand thus obtain\n\\[\\breve{A} = V^{-\\dagger}(\\overline{F}\\odot (V^\\dagger \\breve{V}) + \\mathcal{P}_{D}(\\breve{D}))V^{\\dagger}\\]"
  },
  {
    "objectID": "main.html#schur-decomposition",
    "href": "main.html#schur-decomposition",
    "title": "Automatic differentation rules for linear algebra",
    "section": "Schur decomposition",
    "text": "Schur decomposition\nAnother decomposition that is closely related to the eigenvalue decomposition of general matrices \\(A\\in{\\mathbb{C}}^{n \\times n}\\) is the Schur decomposition, namely\n\\[A = UTU^\\dagger\\]\nwhere now \\(U\\in {\\mathbb{C}}^{n\\times n}\\) is unitary, and \\(T\\in{\\mathbb{C}}^{n\\times n}\\) is upper triangular. In the case of a hermitian matrix \\(A\\), the hermiticity of \\(T\\) requires that also its entries above the diagonal vanish, so that the Schur decomposition then coincides with the eigenvalue decomposition. As in the hermitian eigenvalue decomposition, the phase of the columns of \\(U\\) is not fixed by this equation. However, in this case, a phase change in the columns of \\(U\\) also affects the off-diagonal (=above diagonal) elements of \\(T\\).\nFor the forward rule, we again parameterise \\(\\dot{U}=U\\dot{K}\\) with \\(\\dot{K}=-\\dot{K}^\\dagger\\) in order to find\n\\[\\dot{A}U + A\\dot{U} = \\dot{U}T+ U\\dot{T}\\]\nor thus\n\\[U^\\dagger \\dot{A}U = \\dot{K}T-T\\dot{K} + \\dot{T}\\]\nFrom the gauge freedom, it follows that the diagonal elements of \\(\\dot{K}\\) do not appear in this equations, and can be put to zero (but we should have that the imaginary diagonal components of \\(U^\\dagger \\breve{U}\\) should be zero for the adjoint variables associated with a gauge invariant objective function). We can interpret the equation above as a black-box linear system for the below-diagonal entries of \\(\\dot{K}\\) (which also fix the above-diagonal entries via antihermiticity) and the entries of \\(\\dot{T}\\) and feed it into a linear solver. Note that we should then actually separate everything into real and imaginary components.\nLet us instead try to find some more structure in these equations. We first focus on the entries \\(1 \\leq j &lt; i \\leq n\\), for which \\(\\dot{T}_{ij} = 0\\). We then find\n\\[(U^\\dagger\\dot{A}U)_{ij} = \\sum_{k=1}^j \\dot{K}_{ik} T_{kj} - \\sum_{k=i}^n T_{ik} \\dot{K}_{kj}\\]\n… to be continued …"
  },
  {
    "objectID": "main.html#invariant-subspaces",
    "href": "main.html#invariant-subspaces",
    "title": "Automatic differentation rules for linear algebra",
    "section": "Invariant subspaces",
    "text": "Invariant subspaces\nConsider a matrix \\(A\\in{\\mathbb{C}}^{n \\times n}\\), and a matrix \\(V \\in {\\mathbb{C}}^{n \\times r}\\) of which the columns span an invariant subspace of \\(A\\). This means that it is such that there exists a matrix \\(B \\in {\\mathbb{C}}^{r \\times r}\\) such that we have an exact equality\n\\[AV = VB\\]\nThe matrix \\(V\\) could be composed out of a number of eigenvectors of \\(A\\), in which case \\(B\\) would be diagonal and contain the corresponding eigenvalues. Alternatively, we can choose the columns of \\(V\\) to be orthonormal, i.e. \\(V\\) isometric, and at the same time \\(B\\) upper triangular. We then have a partial Schur factorisation. As always, both choices coincide in the case where \\(A\\) is hermitian."
  },
  {
    "objectID": "main.html#single-eigenvector",
    "href": "main.html#single-eigenvector",
    "title": "Automatic differentation rules for linear algebra",
    "section": "Single eigenvector",
    "text": "Single eigenvector\nWe start with the case where \\(r=1\\). A one-dimensional invariant subspace needs to be an eigenvector. A single eigenvector \\(\\boldsymbol{u}\\) and associated eigenvalue \\(\\lambda\\) satisfy\n\\[A\\boldsymbol{u} = \\lambda \\boldsymbol{u}\\]\nfrom which we obtain\n\\[\\dot{A}\\boldsymbol{u} + A\\boldsymbol{\\dot{u}} = \\dot{\\lambda} \\boldsymbol{u} + \\lambda \\boldsymbol{\\dot{u}}\\]\nSince a single eigenvector can always be chosen normalised, we now set \\(\\boldsymbol{\\dot{u}} = \\boldsymbol{u} k + \\boldsymbol{\\dot{l}}\\) where \\(k\\) is purely imaginary and \\(\\boldsymbol{\\dot{l}}\\) is orthogonal to \\(\\boldsymbol{u}\\). The scalar \\(k\\) corresponds to changes in the phase of the eigenvector, and will disappear from the equations. It cannot be uniquely determined and so it should also be annihilated by the adjoint associated with any gauge-invariant objective function, i.e. it should hold for any purely imaginary \\(k\\) that\n\\[\\mathop{\\mathrm{Re}}(\\boldsymbol{\\breve{u}}^\\dagger \\boldsymbol{u} k) = 0\\quad \\Rightarrow\\quad \\mathop{\\mathrm{Im}}(\\boldsymbol{\\breve{u}}^\\dagger \\boldsymbol{u}) = 0\\]\nIf we thus choose \\(\\dot{k}=0\\), we find that \\(\\boldsymbol{\\dot{u}}=\\boldsymbol{\\dot{l}}\\) lives in the orthogonal complement of \\(\\boldsymbol{u}\\), i.e. \\(\\boldsymbol{u}^\\dagger \\boldsymbol{\\dot{u}}=0\\). By projecting the equation above onto \\(\\boldsymbol{u}\\) and its orthogonal complement, we then find \\[\\begin{align}\n\\dot{\\lambda} &= \\boldsymbol{u}^\\dagger \\dot{A}\\boldsymbol{u} + \\boldsymbol{u}^\\dagger A\\boldsymbol{\\dot{u}} \\\\\n\\lambda \\boldsymbol{\\dot{u}} - (I- \\boldsymbol{u}\\boldsymbol{u}^\\dagger)A\\boldsymbol{\\dot{u}} &= (\\lambda I- A)\\boldsymbol{\\dot{u}} + \\boldsymbol{u}\\boldsymbol{u}^\\dagger A\\boldsymbol{\\dot{u}}= (I- \\boldsymbol{u}\\boldsymbol{u}^\\dagger) \\dot{A} \\boldsymbol{u}\\\\\n\\end{align}\\] When \\(A\\) is hermitian, \\(\\boldsymbol{u}^\\dagger\\) is also a left eigenvector and we can omit the terms containing \\(\\boldsymbol{u}^\\dagger A\\boldsymbol{\\dot{u}}\\) in the first and second equation. In particular, we recover the well-known result \\(\\dot{\\lambda}=\\boldsymbol{u}^\\dagger \\dot{A}\\boldsymbol{u}\\). However, we here deal with the more general case.\nAnother way to write this linear system is as \\[\\begin{align}\n\\begin{bmatrix}\n\\lambda I- A  & \\boldsymbol{u}\\\\\n\\boldsymbol{u}^\\dagger & 0\n\\end{bmatrix}\\begin{bmatrix} \\boldsymbol{\\dot{u}} \\\\ \\dot{\\lambda}\\end{bmatrix}\n= \\begin{bmatrix} \\dot{A}\\boldsymbol{u}\\\\ 0 \\end{bmatrix}\n\\end{align}\\]\nDespite the appearance of \\((\\lambda I-A)\\), which has an eigenvalue zero, the coefficient matrix in this system can be inverted (provided the multiplicity of eigenvalue \\(\\lambda\\) of \\(A\\) is one). This follows from writing it as\n\\[\\begin{align}\n\\begin{bmatrix}\n\\lambda I- A  & \\boldsymbol{u}\\\\\n\\boldsymbol{u}^\\dagger & 0\n\\end{bmatrix}=\\begin{bmatrix}\n\\lambda I- A - \\frac{1}{a} \\boldsymbol{u}\\boldsymbol{u}^\\dagger  & \\boldsymbol{o}\\\\\n\\boldsymbol{o}^\\dagger & -a\n\\end{bmatrix} + \\frac{1}{a} \\begin{bmatrix} \\boldsymbol{u} \\\\ a\\end{bmatrix}\\begin{bmatrix} \\boldsymbol{u}^\\dagger & a \\end{bmatrix}\n\\end{align}\\] where \\(a\\) is an arbitrary nonzero number. When \\(A\\) is hermitian, it is useful to also restrict \\(a\\) to be real, so that both terms in this sum are hermitian.\nAssuming that the eigenvalue \\(\\lambda\\) has multiplicity one, the matrix \\(\\lambda I- A-\\frac{1}{a} \\boldsymbol{u}\\boldsymbol{u}^\\dagger\\) is invertible for any \\(a \\neq \\infty\\), since the spectrum of \\(A+\\frac{1}{a} \\boldsymbol{u}\\boldsymbol{u}^\\dagger\\) is given by the spectrum of \\(A\\) with eigenvalue \\(\\lambda\\) replaced by \\(\\lambda+\\frac{1}{a}\\). Indeed, we find that \\(\\boldsymbol{u}\\) is still a right eigenvector as\n\\[ (A+\\frac{1}{a} \\boldsymbol{u}\\boldsymbol{u}^\\dagger)\\boldsymbol{u} = \\lambda \\boldsymbol{u} + \\frac{1}{a} \\boldsymbol{u}(\\boldsymbol{u}^\\dagger \\boldsymbol{u})\\]\nand \\(\\boldsymbol{u}\\) was assumed normalised to one. Note that this does not require that \\(A\\) is hermitian. That the other eigenvalues \\(\\tilde{\\lambda}\\neq \\lambda\\) are not affected by this extra term, follows from applying the left eigenvector \\(\\boldsymbol{\\tilde{v}}^\\dagger\\) associated with those eigenvalues, which satisfy \\(\\boldsymbol{\\tilde{v}}^\\dagger\\boldsymbol{u}=0\\). However, in the general non-Hermitian case, both the left eigenvector for eigenvalue \\(\\lambda\\) and the right eigenvectors associated with eigenvalues \\(\\tilde{\\lambda}\\neq \\lambda\\) are affected by this extra term.\nWe can then compute the inverse of this matrix using the Sherman-Morisson formula, as the second term is a rank-1 update. We find\n\\[\\begin{align}\n&\\begin{bmatrix}\n\\lambda I- A  & \\boldsymbol{u}\\\\\n\\boldsymbol{u}^\\dagger & 0\n\\end{bmatrix}^{-1} \\\\\n&=\\begin{bmatrix}\n(\\lambda I- A - \\frac{1}{a} \\boldsymbol{u}\\boldsymbol{u}^\\dagger)^{-1}  & \\boldsymbol{o}\\\\\n\\boldsymbol{o}^\\dagger & -\\frac{1}{a} \\end{bmatrix} - \\frac{1}{a} \\frac{\\begin{bmatrix}\n(\\lambda I- A - \\frac{1}{a} \\boldsymbol{u}\\boldsymbol{u}^\\dagger)^{-1}  & \\boldsymbol{o}\\\\\n\\boldsymbol{o}^\\dagger & -\\frac{1}{a} \\end{bmatrix}  \\begin{bmatrix} \\boldsymbol{u} \\\\ a\\end{bmatrix}\\begin{bmatrix} \\boldsymbol{u}^\\dagger & a \\end{bmatrix}  \\begin{bmatrix}\n(\\lambda I- A - \\frac{1}{a} \\boldsymbol{u}\\boldsymbol{u}^\\dagger)^{-1}  & \\boldsymbol{o}\\\\\n\\boldsymbol{o}^\\dagger & -\\frac{1}{a} \\end{bmatrix}}{1+ \\frac{1}{a} \\begin{bmatrix} \\boldsymbol{u}^\\dagger & a \\end{bmatrix} \\begin{bmatrix}\n(\\lambda I- A - \\frac{1}{a} \\boldsymbol{u}\\boldsymbol{u}^\\dagger)^{-1}  & \\boldsymbol{o}\\\\\n\\boldsymbol{o}^\\dagger & -\\frac{1}{a} \\end{bmatrix}   \\begin{bmatrix} \\boldsymbol{u} \\\\ a\\end{bmatrix}}\\\\\n\n&=\\begin{bmatrix}\n(\\lambda I- A - \\frac{1}{a} \\boldsymbol{u}\\boldsymbol{u}^\\dagger)^{-1}  & \\boldsymbol{o}\\\\\n\\boldsymbol{o}^\\dagger & -\\frac{1}{a} \\end{bmatrix} - \\frac{1}{a} \\frac{ \\begin{bmatrix} -a\\boldsymbol{u} \\\\ -1\\end{bmatrix}\\begin{bmatrix}  \\boldsymbol{u}^\\dagger(\\lambda I- A - \\frac{1}{a} \\boldsymbol{u}\\boldsymbol{u}^\\dagger)^{-1} & -1 \\end{bmatrix}}{1+ \\frac{1}{a} \\begin{bmatrix} \\boldsymbol{u}^\\dagger & a \\end{bmatrix} \\begin{bmatrix} -a\\boldsymbol{u} \\\\ -1\\end{bmatrix}}\\\\\n\n&=\\begin{bmatrix}\n(\\lambda I- A - \\frac{1}{a} \\boldsymbol{u}\\boldsymbol{u}^\\dagger)^{-1}  & \\boldsymbol{o}\\\\\n\\boldsymbol{o}^\\dagger & -\\frac{1}{a} \\end{bmatrix} -  \\begin{bmatrix} \\boldsymbol{u} \\\\ \\frac{1}{a}\\end{bmatrix}\\begin{bmatrix}  \\boldsymbol{u}^\\dagger(\\lambda I- A - \\frac{1}{a} \\boldsymbol{u}\\boldsymbol{u}^\\dagger)^{-1} & -1 \\end{bmatrix}\\\\\n\n&=\\begin{bmatrix}\n(I-\\boldsymbol{u}\\boldsymbol{u}^\\dagger)(\\lambda I- A - \\frac{1}{a} \\boldsymbol{u}\\boldsymbol{u}^\\dagger)^{-1}  & \\boldsymbol{u}\\\\\n-\\frac{1}{a}\\boldsymbol{u}^\\dagger(\\lambda I- A - \\frac{1}{a} \\boldsymbol{u}\\boldsymbol{u}^\\dagger)^{-1} & 0 \\end{bmatrix}\n\\end{align}\\]\nHere, we have used that \\((\\lambda I- A - \\frac{1}{a} \\boldsymbol{u}\\boldsymbol{u}^\\dagger)^{-1}\\boldsymbol{u} = -a \\boldsymbol{u}\\), but we have not assumed that \\(A\\) is hermitian. If that is the case, then also the entry in the bottom left corner simplifies down to \\(\\boldsymbol{u}^\\dagger\\).\nInserting this inverse in the linear system, we now find\n\\[\\begin{align}\n\\begin{bmatrix}\n\\boldsymbol{\\dot{u}}\\\\\n\\dot{\\lambda}\n\\end{bmatrix} = \\begin{bmatrix} (I-\\boldsymbol{u}\\boldsymbol{u}^\\dagger)(\\lambda I- A - \\frac{1}{a} \\boldsymbol{u}\\boldsymbol{u}^\\dagger)^{-1} \\dot{A}\\boldsymbol{u}\\\\\n-\\frac{1}{a}\\boldsymbol{u}^\\dagger(\\lambda I- A - \\frac{1}{a} \\boldsymbol{u}\\boldsymbol{u}^\\dagger)^{-1}\\dot{A}\\boldsymbol{u}\n\\end{bmatrix}\n\\end{align}\\]\nNote that the equation\n\\[\\dot{\\lambda} = -\\frac{1}{a}\\boldsymbol{u}^\\dagger(\\lambda I- A - \\frac{1}{a} \\boldsymbol{u}\\boldsymbol{u}^\\dagger)^{-1}\\dot{A}\\boldsymbol{u}\\]\nreduced to its well known form \\(\\dot{\\lambda}=\\boldsymbol{u}^\\dagger \\dot{A}\\boldsymbol{u}\\) in the case where \\(A\\) is Hermitian. One might have expected the nonhermitian generalisation \\(\\dot{\\lambda}=\\boldsymbol{v}^\\dagger \\dot{A}\\boldsymbol{u}\\) where \\(\\boldsymbol{v}^\\dagger\\) is the left eigenvector of \\(A\\) corresponding to eigenvalue \\(\\lambda\\). In fact, it turns out that it holds indeed that\n\\[\\boldsymbol{v}^\\dagger = -\\frac{1}{a}\\boldsymbol{u}^\\dagger(\\lambda I- A - \\frac{1}{a} \\boldsymbol{u}\\boldsymbol{u}^\\dagger)^{-1}\\]\nas can be verified by writing it as\n\\[\\boldsymbol{v}^\\dagger(\\lambda I- A) - \\frac{1}{a} (\\boldsymbol{v}^\\dagger \\boldsymbol{u})\\boldsymbol{u^\\dagger} = -\\frac{1}{a}\\boldsymbol{u}^\\dagger\\]\nIndeed, it is exactly the left eigenvector \\(\\boldsymbol{v}\\), normalised such that \\(\\boldsymbol{v}^\\dagger \\boldsymbol{u}=1\\), that satisfies this equation. However, instead of explicitly needing to know this left eigenvector, either by separately solving the eigenvalue equation for the left eigenvector, or by solving the linear system above, we only need to solve one linear system, namely\n\\[(\\lambda I- A - \\frac{1}{a} \\boldsymbol{u}\\boldsymbol{u}^\\dagger)^{-1}\\dot{A}\\boldsymbol{u}\\]\nin order to determine both \\(\\boldsymbol{\\dot{u}}\\) and \\(\\dot{\\lambda}\\). Note, finally, that while the precise choice of \\(a\\) was irrelevant in all of the above, in practice it might affect the condition number of the matrix that needs to be inverted, and it can be beneficial to make a careful choice, based on what properties of \\(A\\) are known.\nFinally, we derive the backward rule, via\n\\[\\begin{align}\n\\mathop{\\mathrm{Re}}(\\boldsymbol{\\breve{u}}^\\dagger \\boldsymbol{\\dot{u}})+\\mathop{\\mathrm{Re}}(\\breve{\\lambda}^\\dagger \\dot{\\lambda}) &=\\mathop{\\mathrm{Re}}([(I-\\boldsymbol{u}\\boldsymbol{u}^\\dagger)\\boldsymbol{\\breve{u}} - \\frac{1}{\\overline{a}}\\breve{\\lambda}\\boldsymbol{u}]^\\dagger(\\lambda I- A - \\frac{1}{a} \\boldsymbol{u}\\boldsymbol{u}^\\dagger)^{-1}\\dot{A}\\boldsymbol{u})\\\\\n&=\\mathop{\\mathrm{RTr}}(((\\overline{\\lambda}I-A^\\dagger -\\frac{1}{\\overline{a}}\\boldsymbol{u}\\boldsymbol{u}^\\dagger)^{-1}[(I-\\boldsymbol{u}\\boldsymbol{u}^\\dagger)\\boldsymbol{\\breve{u}} - \\frac{1}{\\overline{a}}\\breve{\\lambda}\\boldsymbol{u}]\\boldsymbol{u}^\\dagger)^\\dagger \\dot{A})\n\\end{align}\\]\nfrom which we obtain\n\\[\\breve{A} = (\\overline{\\lambda}I-A^\\dagger -\\frac{1}{\\overline{a}}\\boldsymbol{u}\\boldsymbol{u}^\\dagger)^{-1}[(I-\\boldsymbol{u}\\boldsymbol{u}^\\dagger)\\boldsymbol{\\breve{u}} - \\frac{1}{\\overline{a}}\\breve{\\lambda}\\boldsymbol{u}]\\boldsymbol{u}^\\dagger\\]"
  },
  {
    "objectID": "main.html#single-eigenvector-alternative",
    "href": "main.html#single-eigenvector-alternative",
    "title": "Automatic differentation rules for linear algebra",
    "section": "Single eigenvector (alternative)",
    "text": "Single eigenvector (alternative)\nThe previous scheme of course hints towards an alternative scheme, that is probably better known or more expected, for the case of a nonhermitian matrix \\(A\\) for which both the left eigenvector \\(\\boldsymbol{u}\\) and the right eigenvector \\(\\boldsymbol{v}^\\dagger\\) associated with an eigenvalue \\(\\lambda\\) are known. It is furthermore assumed that \\(\\lambda\\) has (algebraic and geometric) multiplicity one, so that there are no Jordan blocks and \\(\\boldsymbol{v}^\\dagger \\boldsymbol{u}\\neq 0\\). We then normalise \\(\\boldsymbol{v}^\\dagger\\) such that \\(\\boldsymbol{v}^\\dagger \\boldsymbol{u}=1\\). If we are only interested in the chain rule with respect to \\(\\boldsymbol{u}\\), we can now gauge the tangent direction so that \\(\\boldsymbol{v}^\\dagger \\boldsymbol{\\dot{u}} =0\\), giving rise to the linear system\n\\[\\begin{align}\n\\begin{bmatrix}\n\\lambda I- A  & \\boldsymbol{u}\\\\\n\\boldsymbol{v}^\\dagger & 0\n\\end{bmatrix}\\begin{bmatrix} \\boldsymbol{\\dot{u}} \\\\ \\dot{\\lambda}\\end{bmatrix}\n= \\begin{bmatrix} \\dot{A}\\boldsymbol{u}\\\\ 0 \\end{bmatrix}\n\\end{align}\\]\nMost of the analysis from the previous subsection can easily be generalised by substituting \\(\\boldsymbol{u}^\\dagger\\) with \\(\\boldsymbol{v}^\\dagger\\). But since now \\(\\boldsymbol{v}^\\dagger\\) is a left eigenvector of \\(A\\), and also of \\(A + \\frac{1}{a} \\boldsymbol{u}\\boldsymbol{v}^\\dagger\\), some simplification occurs. In this case, the spectral (or Jordan) decomposition of \\(A + \\frac{1}{a} \\boldsymbol{u}\\boldsymbol{v}^\\dagger\\) coincides completely with that of \\(A\\) (both eigenvalues and left and right eigenvectors), except for eigenvalue \\(\\lambda\\) being shifted to \\(\\lambda + \\frac{1}{a}\\). We obtain as final result\n\\[\\begin{align}\n\\begin{bmatrix}\n\\boldsymbol{\\dot{u}}\\\\\n\\dot{\\lambda}\n\\end{bmatrix} = \\begin{bmatrix} (I-\\boldsymbol{u}\\boldsymbol{v}^\\dagger)(\\lambda I- A - \\frac{1}{a} \\boldsymbol{u}\\boldsymbol{v}^\\dagger)^{-1} \\dot{A}\\boldsymbol{u}\\\\\n\\boldsymbol{v}^\\dagger\\dot{A}\\boldsymbol{u}\n\\end{bmatrix}\n\\end{align}\\]\nFor the reverse rule, we now obtain\n\\[\\breve{A} = (\\overline{\\lambda}I-A^\\dagger -\\frac{1}{\\overline{a}}\\boldsymbol{v}\\boldsymbol{u}^\\dagger)^{-1}(I-\\boldsymbol{u}\\boldsymbol{v}^\\dagger)\\boldsymbol{\\breve{u}}  + \\breve{\\lambda}\\boldsymbol{v}\\boldsymbol{u}^\\dagger\\]\nNote that this alternative method is useful if the left eigenvector is already known. If however it still needs to be computed, the previous approach is expected to be computationally cheaper, as it only requires solving one linear system. In the case where \\(A\\) is hermitian and thus \\(v=u\\), these two approaches coincides."
  },
  {
    "objectID": "main.html#higher-dimensional-invariant-subspace",
    "href": "main.html#higher-dimensional-invariant-subspace",
    "title": "Automatic differentation rules for linear algebra",
    "section": "Higher-dimensional invariant subspace",
    "text": "Higher-dimensional invariant subspace\nNow let us consider the case where \\(V\\in{\\mathbb{C}}^{n \\times p}\\) and contains \\(p\\) eigenvectors \\({v}_i\\) and associated eigenvectors \\(\\lambda_i\\) for \\(i=1,\\ldots,p\\). Of course, we can just reuse the result from the previous subsection for each pair \\((\\lambda_i,{v}_i)\\) separately. For the reverse rule, because of linearity, we can then simply add the results, i.e.\n\\[\\begin{align}\n\\breve{A} = \\sum_{i=1}^{p}(\\overline{\\lambda}_iI-A^\\dagger -\\frac{1}{\\overline{a}}\\boldsymbol{v}_i\\boldsymbol{v}_i^\\dagger)^{-1}[(I-\\boldsymbol{v}_i\\boldsymbol{v}_i^\\dagger)\\boldsymbol{\\breve{v}_i} - \\frac{1}{\\overline{a}}\\breve{\\lambda}_i\\boldsymbol{v}_i]\\boldsymbol{v}_i^\\dagger\n\\end{align}\\]\nHowever, it might be that there are alternative approaches.\n\nHermitian case\nLet us first focus on the case where \\(A\\) is hermitian, as in this case the matrix \\(V\\) satisfies the additional constraint that it is isometric, so that it columns are not independent.\nWe can then parameterise \\(\\dot{V}=V\\dot{K}+\\dot{L}\\) where \\(\\dot{K}\\) is antihermitian and \\(V^\\dagger \\dot{L} = O\\). We insert this decomposition in\n\\[\\dot{A}V + A\\dot{V}=\\dot{V}D+V\\dot{D}\\]\nand project the resulting equation onto \\(V\\) and its orthogonal complement, in order to obtain\n\\[\\begin{align}\nV^\\dagger \\dot{A}V &= \\dot{K}D-D\\dot{K} + \\dot{D}\\\\\n(I- VV^\\dagger)\\dot{A}V &=  \\dot{L}D -(I- VV^\\dagger)A\\dot{L}=-\\mathcal{S}_{(I- VV^\\dagger)A,D}(\\dot{L})\n\\end{align}\\]\nThe first equation is identical to that of a full eigenvalue decomposition, so that we can recycle the results from that section. For the Sylvester equation on the second line, one might be inclined to omit the projection \\((I- VV^\\dagger)\\) in front of \\(A\\dot{L}\\), as \\(V^\\dagger A\\dot{L}=DV^\\dagger\\dot{L}\\), and \\(V^\\dagger\\dot{L}\\) is supposed to be zero. However, it is exactly the presence of this factor that will enforce that \\(\\dot{L}\\) will satisfy this constraint. A different perspective is that the projection is necessary to ensure \\((I-VV^\\dagger)A\\) and \\(D\\) do not have common eigenvalues, which makes that the Sylvester equation will have a unique solution. In fact, this requires that all eigenvalues with higher multiplicity are either completely contained in the subspace \\(V\\) or in its orthogonal complement. Furthermore, it is also important that none of the eigenvalues in \\(V\\), or thus of the diagonal elements in \\(D\\), are zero. If this is the case, it can help to regularise the second equation as\n\\[\\begin{align}\n(I- VV^\\dagger)\\dot{A}V &=  \\dot{L}D -A\\dot{L} - VD'V^\\dagger \\dot{L}\n\\end{align}\\]\nwhere \\(D'\\) is a diagonal matrix that is chosen such that \\(D\\) and \\(D+D'\\) do not have any eigenvalues (diagonal elements) in common. If \\(D\\) does not contain eigenvalues zero, then the choice \\(D'=-D\\) reduces this equation to its original form.\nWe now find the solution\n\\[\\begin{align}\n\\dot{K} &= F \\odot (V^\\dagger \\dot{A}V)\\\\\n\\dot{D} &= \\mathcal{P}_{rD}(V^\\dagger \\dot{A}V)\\\\\n\\dot{L} &= - \\mathcal{S}_{A+VD'V^\\dagger , D}^{-1}((I- VV^\\dagger)\\dot{A}V)\n\\end{align}\\] with \\(F_{ij}=\\begin{cases} \\frac{1}{\\lambda_j-\\lambda_i},& i\\neq j\\\\ 0,&i=j\\end{cases}\\) and \\(\\lambda_i = D_{ii} \\in {\\mathbb{R}}\\).\nFor the reverse rule, we insert \\(\\dot{V}=V\\dot{K} + (I- VV^\\dagger)\\dot{L}\\), where the explicit projector onto the orthogonal complement of \\(V\\) in the second term does not affect \\(\\dot{L}\\), but will help in projecting the corresponding component out of the adjoint variable \\(\\breve{V}\\). We then find \\[\\begin{align}\n\\mathop{\\mathrm{RTr}}(\\breve{V}^\\dagger \\dot{V})&+\\mathop{\\mathrm{RTr}}(\\breve{D}^\\dagger \\dot{D})\\\\\n&=\\mathop{\\mathrm{RTr}}([(V^\\dagger\\breve{V})\\odot F + \\mathcal{P}_{rD}(\\breve{D})]^\\dagger V^\\dagger\\dot{A}V) + \\mathop{\\mathrm{RTr}}([-\\mathcal{S}_{A+VD'V^\\dagger , D}^{-1}((I- VV^\\dagger)\\breve{V})]^\\dagger (I- VV^\\dagger)\\dot{A}V)\\\\\n&= \\mathop{\\mathrm{RTr}}([V\\{(V^\\dagger\\breve{V})\\odot F + \\mathcal{P}_{rD}(\\breve{D})\\}V^\\dagger - (I- VV^\\dagger)\\mathcal{S}_{A+VD'V^\\dagger , D}^{-1}((I- VV^\\dagger)\\breve{V})V^\\dagger]^\\dagger\\dot{A})\n\\end{align}\\]\nwhere we have used the hermiticity of \\(A\\) (and \\(D\\) and \\(D'\\)) in order to bring the (inverse) Sylvester superoperator to the dual variables. We thus find\n\\[\\breve{A}=V[(V^\\dagger\\breve{V})\\odot F + \\mathcal{P}_{rD}(\\breve{D})]V^\\dagger - (I- VV^\\dagger)\\mathcal{S}_{A+VD'V^\\dagger , D}^{-1}((I- VV^\\dagger)\\breve{V})V^\\dagger\\]\nwhere, as mentioned above, \\(D'\\) is a an arbitrary (real) diagonal matrix that is chosen such that \\(D\\) and \\(D+D'\\) do not have any eigenvalues (diagonal elements) in common.\nAlternatively, if we can afford to explicitly compute a complement \\(V_\\perp\\) and parameterise \\(\\dot{L} = V_\\perp \\dot{K}_\\perp\\) and thus \\(\\dot{V}=V\\dot{K}+V_\\perp \\dot{K}_\\perp\\), we would find\n\\[\\begin{align}\n\\dot{K} &= F \\odot (V^\\dagger \\dot{A}V)\\\\\n\\dot{D} &= \\mathcal{P}_{rD}(V^\\dagger \\dot{A}V)\\\\\n\\dot{K}_\\perp &= - \\mathcal{S}_{V_\\perp^\\dagger AV_\\perp, D}^{-1}(V_\\perp^\\dagger\\dot{A}V)\n\\end{align}\\]\nwhere again we assume that \\(A\\) does not have common eigenvalues in the subspace \\(V\\) (diagonal elements of \\(D\\)) and in the subspace \\(V_\\perp\\). This description translates to a reverse rule\n\\[\\breve{A}=V[(V^\\dagger\\breve{V})\\odot F + \\mathcal{P}_{rD}(\\breve{D})]V^\\dagger - V_\\perp\\mathcal{S}_{V_\\perp^\\dagger AV_\\perp , D}^{-1}(V_\\perp^\\dagger\\breve{V})V^\\dagger\\]\n\n\nNon-hermitian case\nFor a non-hermitian matrix \\(A\\), we can still have a subspace spanned by the columns of \\(V\\), where we assume these columns to be different eigenvectors, such that \\(AV=VD\\)\nAs these columns are now not necessarily orthogonal, we first compute their Gram matrix \\(G=V^\\dagger V\\), in order to build an orthogonal projector\n\\[P_{V} = V G^{-1} V^\\dagger\\]\nNote that \\(G^{-1}V^\\dagger\\) is often referred to as the (Moore-Penrose) pseudoinverse of \\(V\\) and is then denoted as \\(V^+\\). In the case where \\(V = {\\mathbb{F}}^{n \\times n}\\), it coincides with \\(V^{-1}\\). In the case where the columns of \\(V\\) are orthogonal, \\(G=I\\) and it coincides with \\(V^\\dagger\\).\nWe can now again parameterise \\(\\dot{V} = V \\dot{K} + \\dot{L}\\), which is simply a way to decompose \\(\\dot{V}\\) into a component along \\(V\\) and a component in the orthogonal complement. Put differently, we have \\(V\\dot{K} = P_{V}\\dot{V}\\) and \\(\\dot{L} = (I- P_{V})\\dot{V}\\), which shows that \\(V^\\dagger \\dot{L} =O\\). In this case, however there is no contraint on \\(\\dot{K}\\). However, as the individual columns of \\(V\\) are only determined up to phases and normalisation, we can still impose a condition on the diagonal of \\(\\dot{K}\\). This is exactly what was used in the full eigenvalue decomposition.\nNow inserting \\(\\dot{V} = V \\dot{K} + \\dot{L}\\) in\n\\[\\dot{A}V + A\\dot{V} = \\dot{V}D + V\\dot{D}\\]\nand projecting onto \\(V\\) and its orthogonal complement, we find\n\\[\\begin{align}\nV^\\dagger\\dot{A}V + GD\\dot{K} + V^\\dagger A \\dot{L} &= G\\dot{K} D + G \\dot{D}\\\\\n(I- VG^{-1}V^\\dagger)\\dot{A}V + (I- VG^{-1}V^\\dagger)A\\dot{L} &= \\dot{L} D\n\\end{align}\\]\nor thus\n\\[\\begin{align}\nG^{-1}V^\\dagger\\left(\\dot{A}V +A \\dot{L}\\right) &= \\dot{K} D - D\\dot{K} + \\dot{D}\\\\\n(I- VG^{-1}V^\\dagger)\\dot{A}V &= \\dot{L} D - (I- VG^{-1}V^\\dagger)A\\dot{L} = - \\mathcal{S}_{(I- VG^{-1}V^\\dagger)A,D}(\\dot{L})\n\\end{align}\\]\nThe first equation defines \\(\\dot{D}\\) and \\(\\dot{K}\\) in terms of the left hand side, which still includes \\(\\dot{L}\\). The latter can be completely determined from the second equation. This second equation, as before, contains a Sylvester operator involving the matrices \\(D\\), the restriction of \\(A\\) onto the subspace \\(V\\), and \\((I-VG^{-1}V^\\dagger)A\\). The second matrix has the same spectrum of \\(A\\), except that all eigenvalues contained in \\(V\\) have been mapped to zero. More generally, we could again regularise this Sylvester equation by replacing \\((I-VG^{-1}V^\\dagger)A\\) with \\(A + VD'G^{-1}V^\\dagger = A+V D'V^+\\), where \\(D'\\) is chosen such that \\(D\\) and \\(D+D'\\) have no eigenvalues in common. Note that now both choices \\((I-VG^{-1}V^\\dagger)A\\) and \\(A + VD'G^{-1}V^\\dagger = A+V D'V^+\\) do not become equal for the choice \\(D'=-D\\), even though the effect on the spectrum is the same (right eigenvectors in \\(V\\) are preserved and have eigenvalue zero, complementary eigenvalues are preserved together with their left eigenvectors). To streamline the rest of the discussion, I will denote either of these choises as \\(A'\\).\nWe now find the solution\n\\[\\begin{align}\n\\dot{L} &= - \\mathcal{S}_{A', D}^{-1}((I- VV^+)\\dot{A}V)=- \\mathcal{S}_{A', D}^{-1}((I- P_V)\\dot{A}V)\\\\\n\\dot{K} &= F \\odot (V^+ \\dot{A}V + V^+ A\\dot{L}) = F \\odot (V^+ \\dot{A}V - V^+ A\\mathcal{S}_{A', D}^{-1}((I- P_V)\\dot{A}V))\\\\\n\\dot{D} &= \\mathcal{P}_{D}(V^+ \\dot{A}V + V^+ A\\dot{L})=\\mathcal{P}_{D}(V^+ \\dot{A}V - V^+ A\\mathcal{S}_{A', D}^{-1}((I- P_V)\\dot{A}V))\\\\\n\\end{align}\\]\nwith, as before, \\(F_{ij}=\\begin{cases} \\frac{1}{\\lambda_j-\\lambda_i},& i\\neq j\\\\ 0,&i=j\\end{cases}\\) and \\(\\lambda_i = D_{ii}\\).\nFor the reverse rule, we then start from\n\\[\\begin{align}\n\\mathop{\\mathrm{RTr}}(\\breve{V}^\\dagger \\dot{V}) &+ \\mathop{\\mathrm{RTr}}(\\breve{D}\\dot{D}) \\\\\n&= \\mathop{\\mathrm{RTr}}([(V^\\dagger \\breve{V}) \\odot {\\overline{F}} + \\mathcal{P}_{D}(\\breve{D})]^\\dagger [ V^+ \\dot{A}V + V^+ A\\dot{L}]) + \\mathop{\\mathrm{RTr}}(((I- P_V)\\breve{V})^\\dagger\\dot{L})\n\\end{align}\\]\nand using the shorthand \\[\\begin{align}\nZ &= (V^+)^\\dagger[(V^\\dagger \\breve{V}) \\odot {\\overline{F}} + \\mathcal{P}_{D}(\\breve{D})] = V G^{-1} [(V^\\dagger \\breve{V}) \\odot {\\overline{F}} + \\mathcal{P}_{D}(\\breve{D})]\n\\end{align}\\]\nwe find\n\\[\\begin{align}\n\\mathop{\\mathrm{RTr}}(\\breve{V}^\\dagger \\dot{V}) &+ \\mathop{\\mathrm{RTr}}(\\breve{D}\\dot{D}) \\\\\n&= \\mathop{\\mathrm{RTr}}(Z^\\dagger \\dot{A}V) + \\mathop{\\mathrm{RTr}}([(I- P_V)\\breve{V} + A^\\dagger Z]^\\dagger \\dot{L})\\\\\n&= \\mathop{\\mathrm{RTr}}(Z^\\dagger \\dot{A}V) - \\mathop{\\mathrm{RTr}}([(I- P_V)\\breve{V} + A^\\dagger Z]^\\dagger\\mathcal{S}_{A', D}^{-1}((I- P_V)\\dot{A}V))\\\\\n&= \\mathop{\\mathrm{RTr}}(Z^\\dagger \\dot{A}V) - \\mathop{\\mathrm{RTr}}((\\mathcal{S}_{A'^\\dagger,D^\\dagger}^{-1}((I- P_V)\\breve{V} + A^\\dagger Z))^\\dagger((I- P_V)\\dot{A}V))\\\\\n&= \\mathop{\\mathrm{RTr}}([Z V^\\dagger - (I- P_V) \\mathcal{S}^{-1}_{A'^\\dagger,D^\\dagger}((I-P_V)\\breve{V}+A^\\dagger Z)V^\\dagger]^\\dagger \\dot{A})\n\\end{align}\\]\nin order to find\n\\[\\begin{align}\n\\breve{A}=Z V^\\dagger - (I- P_V) \\mathcal{S}^{-1}_{A'^\\dagger,D^\\dagger}((I-P_V)\\breve{V}+A^\\dagger Z)V^\\dagger\n\\end{align}\\]"
  },
  {
    "objectID": "main.html#full-rank-decomposition",
    "href": "main.html#full-rank-decomposition",
    "title": "Automatic differentation rules for linear algebra",
    "section": "Full rank decomposition",
    "text": "Full rank decomposition\nWe start with the case of a square matrix (\\(m=n\\)) that has full rank (\\(r=m\\)), for which we consider the full singular value decomposition.\nTo derive the forward rule, we write\n\\[\\dot{A} V + A\\dot{V} = \\dot{U}S + U\\dot{S}\\]\nand parameterise \\(\\dot{U}= U\\dot{K}\\) and \\(\\dot{V}=V\\dot{M}\\) with \\(\\dot{K}\\) and \\(\\dot{M}\\) are antihermitian. We then find\n\\[U^\\dagger \\dot{A} V =  \\dot{K} S - S\\dot{M} + \\dot{S}\\]\nWriting the components of these equations for the diagonal and off-diagonal separately, we obtain\n\\[\\begin{align}\n(U^\\dagger \\dot{A} V)_{ii} &= s_i(\\dot{K}_{ii} -\\dot{M}_{ii}) + \\dot{S}_{ii}\\\\\n(U^\\dagger \\dot{A} V)_{ij} &= \\dot{K}_{ij} s_j -\\dot{M}_{ij} s_i, \\qquad i\\neq j\n\\end{align}\\]\nBecause the antihermitian matrices \\(\\dot{K}\\) and \\(\\dot{M}\\) only have imaginary diagonal entries, we immediately obtain \\(\\dot{S}=\\mathcal{P}_{rD}(U^\\dagger \\dot{A} V)\\). For the imaginary part of the diagaonal components, we cannot unambiguously distribute them over \\(\\dot{K}_{ii}\\) and \\(\\dot{M}_{ii}\\) because of the gauge invariance, which means that only there difference is fixed. This implies that any adjoint variables resulting from a gauge invariant objective function should satisfy \\(\\mathop{\\mathrm{RTr}}(\\breve{U}^\\dagger \\dot{U} + \\breve{V}^\\dagger \\dot{V})=0\\) whenever \\(\\dot{U} = U\\dot{D}\\) and \\(\\dot{V}=V\\dot{D}\\) for the same purely imaginary and diagonal matrix \\(\\dot{D}\\), which we could express as the condition\n\\[\\mathcal{P}_{iD}(U^\\dagger \\breve{U} + V^\\dagger\\breve{V})=0\\]\nfor the adjoint variables associated with a gauge-invariant cost function. We can generalise this constraint to the case where some singular values coincide.\nWe now make an arbitrary gauge choice that\n\\[\\dot{K}_{ii} = - \\dot{M}_{ii}= \\frac{{\\mathrm{i}}}{2} \\frac{\\mathop{\\mathrm{Im}}((U^\\dagger \\dot{A}V)_{ii})}{s_i} = \\frac{(U^\\dagger \\dot{A}V)_{ii} - (V^\\dagger \\dot{A}^\\dagger U)_{ii}}{4 s_i}\\]\nFor the off-diagonal components of \\(\\dot{K}\\) and \\(\\dot{M}\\), we parameterise the components above the diagonal in terms of those below the diagonal, in order to find\n\\[\\begin{align}\n(U^\\dagger \\dot{A} V)_{ij} &= \\dot{K}_{ij} s_j -\\dot{M}_{ij} s_i, \\qquad i &gt; j\\\\\n(U^\\dagger \\dot{A} V)_{ij} &= -\\overline{\\dot{K}_{ji}} s_j +\\overline{\\dot{M}_{ji}} s_i, \\qquad i&lt; j\n\\end{align}\\]\nor thus, for all \\(i &gt; j\\): \\[\\begin{align}\n(U^\\dagger \\dot{A} V)_{ij} &= \\dot{K}_{ij} s_j -\\dot{M}_{ij} s_i\\\\\n-(V^\\dagger \\dot{A}^\\dagger U)_{ij} &= \\dot{K}_{ij} s_i -\\dot{M}_{ij} s_j\n\\end{align}\\] from which we can solve \\[\\begin{align}\n\\dot{K}_{ij} &= \\frac{s_j (U^\\dagger \\dot{A} V)_{ij} + s_i (V^\\dagger \\dot{A}^\\dagger U)_{ij}}{s_j^2 - s_i^2} = \\frac{1}{2} \\frac{(U^\\dagger \\dot{A} V)_{ij} + (V^\\dagger \\dot{A}^\\dagger U)_{ij}}{s_j - s_i} + \\frac{1}{2} \\frac{(U^\\dagger \\dot{A} V)_{ij} - (V^\\dagger \\dot{A}^\\dagger U)_{ij}}{s_j + s_i} \\\\\n\\dot{M}_{ij} &= \\frac{s_j (V^\\dagger \\dot{A}^\\dagger U)_{ij} + s_i (U^\\dagger \\dot{A} V)_{ij}}{s_j^2 - s_i^2}=\\frac{1}{2} \\frac{(U^\\dagger \\dot{A} V)_{ij} + (V^\\dagger \\dot{A}^\\dagger U)_{ij}}{s_j - s_i} - \\frac{1}{2} \\frac{(U^\\dagger \\dot{A} V)_{ij} - (V^\\dagger \\dot{A}^\\dagger U)_{ij}}{s_j + s_i}\n\\end{align}\\] While these equations were derived for the entries of \\(\\dot{K}\\) and \\(\\dot{M}\\) with \\(i &gt; j\\), it can easily be verified that the same equations hold for \\(i &lt; j\\). Indeed, the first term on the last right hand side of both equations can be recognised as the Hermitian part \\(\\mathcal{P}_{H}(U^\\dagger \\dot{A}V)\\), which is then made antihermitian by the Hadamard product with \\(F\\), where \\(F_{ij}=\\begin{cases}\\frac{1}{s_j - s_i},& i\\neq j\\\\0,&\\text{otherwise}\\end{cases}\\). The second term of both right hand sides corresponds to the antihermitian part \\(\\mathcal{P}_{A}(U^\\dagger \\dot{A}V)\\), which keeps remains antihermitian upon taking the Hadamard product with \\(G\\), where \\(G_{ij} = \\frac{1}{s_i+s_j}\\). In fact, this last term is even compatible with the specific gauge we have chosen for the diagonal entries of \\(\\dot{K}\\) and \\(\\dot{M}\\), and can thus be taken to hold for all \\(i,j=1,\\ldots,n\\). We then obtain the final result for the forward rules\n\\[\\begin{align}\n\\dot{S} &= \\mathcal{P}_{rD}(U^\\dagger \\dot{A}V) = \\mathcal{P}_{D}(\\mathcal{P}_{H}(U^\\dagger \\dot{A}V))\\\\\n\\dot{U} &= U (F\\odot \\mathcal{P}_{H}(U^\\dagger \\dot{A}V) + G\\odot \\mathcal{P}_{A}(U^\\dagger \\dot{A}V))\\\\\n\\dot{V} &= V (F\\odot \\mathcal{P}_{H}(U^\\dagger \\dot{A}V) - G\\odot \\mathcal{P}_{A}(U^\\dagger \\dot{A}V))\n\\end{align}\\]\nWe can now readily obtain the reverse rule using \\[\\begin{align}\n\\mathop{\\mathrm{RTr}}(\\breve{U}^\\dagger \\dot{U})&+\\mathop{\\mathrm{RTr}}(\\breve{V}^\\dagger \\dot{V})+\\mathop{\\mathrm{RTr}}(\\breve{S}^\\dagger \\dot{S})\\\\\n&= \\mathop{\\mathrm{RTr}}(((U^\\dagger\\breve{U} + V^\\dagger\\breve{V})\\odot F)^\\dagger \\mathcal{P}_H(U^\\dagger \\dot{A}V)) \\\\\n&\\qquad + \\mathop{\\mathrm{RTr}}(((U^\\dagger\\breve{U} - V^\\dagger\\breve{V})\\odot G)^\\dagger \\mathcal{P}_A(U^\\dagger \\dot{A}V))\\\\\n&\\qquad+ \\mathop{\\mathrm{RTr}}(\\mathcal{P}_{rD}(\\breve{S})^\\dagger (U^\\dagger\\dot{A}V))\\\\\n&= \\mathop{\\mathrm{RTr}}((U [ \\mathcal{P}_{rD}(\\breve{S}) + \\mathcal{P}_H((U^\\dagger\\breve{U} + V^\\dagger\\breve{V})\\odot F) + \\mathcal{P}_A((U^\\dagger\\breve{U} - V^\\dagger\\breve{V})\\odot G)]V^\\dagger)^\\dagger\\dot{A})\n\\end{align}\\] from which we conclude that\n\\[\\begin{align}\n\\breve{A} &= U [ \\mathcal{P}_{rD}(\\breve{S}) + \\mathcal{P}_H((U^\\dagger\\breve{U} + V^\\dagger\\breve{V})\\odot F) + \\mathcal{P}_A((U^\\dagger\\breve{U} - V^\\dagger\\breve{V})\\odot G)]V^\\dagger\\\\\n&= U [ \\mathcal{P}_{rD}(\\breve{S}) + \\mathcal{P}_A(U^\\dagger\\breve{U} + V^\\dagger\\breve{V})\\odot F + \\mathcal{P}_A(U^\\dagger\\breve{U} - V^\\dagger\\breve{V})\\odot G]V^\\dagger\\\\\n\\end{align}\\]\nwhere thus\n\\[\\begin{align}\nF_{ij}&=\\begin{cases}\\frac{1}{s_j - s_i},& i\\neq j\\\\0,&\\text{otherwise}\\end{cases},\\\\\nG_{ij} &= \\frac{1}{s_i+s_j}.\n\\end{align}\\]\nUsing \\[\\begin{align}\n(F\\odot G)_{ij}=\\begin{cases}\\frac{1}{s_j^2 - s_i^2},& i\\neq j\\\\0,&\\text{otherwise}\\end{cases}\n\\end{align}\\] we could rewrite this as \\[\\begin{align}\n\\breve{A} &= U [ \\mathcal{P}_{rD}(\\breve{S}) + \\mathcal{P}_A(U^\\dagger\\breve{U} + V^\\dagger\\breve{V})\\odot F + \\mathcal{P}_A(U^\\dagger\\breve{U} - V^\\dagger\\breve{V})\\odot G]V^\\dagger\\\\\n&= U [ \\mathcal{P}_{rD}(\\breve{S}) + (\\{\\mathcal{P}_A(U^\\dagger\\breve{U} + V^\\dagger\\breve{V}), S\\} + [\\mathcal{P}_A(U^\\dagger\\breve{U} - V^\\dagger\\breve{V}),S])\\odot (F \\odot G) + \\mathcal{P}_{iD}(U^\\dagger\\breve{U} - V^\\dagger\\breve{V})\\odot G]V^\\dagger\\\\\n&= U [ \\mathcal{P}_{rD}(\\breve{S}) + (2\\mathcal{P}_A(U^\\dagger\\breve{U})\\cdot S + 2 S \\cdot \\mathcal{P}_A(V^\\dagger\\breve{V}))\\odot (F \\odot G)+ \\mathcal{P}_{iD}(U^\\dagger\\breve{U} - V^\\dagger\\breve{V})\\odot G]V^\\dagger\\\\\n&= U [ \\mathcal{P}_{rD}(\\breve{S}) + 2\\mathcal{P}_H(U^\\dagger\\breve{U}\\odot (F \\odot G))\\cdot S +  S \\cdot 2\\mathcal{P}_H(V^\\dagger\\breve{V}\\odot (F \\odot G))+ \\mathcal{P}_{iD}(U^\\dagger\\breve{U} - V^\\dagger\\breve{V})\\odot G]V^\\dagger\\\\\n\\end{align}\\]\nIt is this last form that appears for example in https://arxiv.org/1909.02659, except that a different gauge for the diagonal elements was chosen in the forward rule, which also alters the final term of the reverse rule. Indeed, in that reference, the gauge for the diagonal elements was chosen such that\n\\[\\dot{K}_{ii} =0, \\qquad  \\dot{M}_{ii}= \\frac{(U^\\dagger \\dot{A}V)_{ii} - (V^\\dagger \\dot{A}^\\dagger U)_{ii}}{2 s_i}\\]\nWhile it might seem suspicious that this affects the reverse rule, note that dual variables associated with gauge invariant objective functions should satisfy\n\\[\\mathcal{P}_{iD}(U^\\dagger \\breve{U} + V^\\dagger\\breve{V})=0\\]\nas mentioned above, so that we could then reexpress\n\\[\\mathcal{P}_{iD}(U^\\dagger\\breve{U} - V^\\dagger\\breve{V}) = -2 \\mathcal{P}_{iD}(V^\\dagger\\breve{V})\\]\nUsing this substitution, our reverse rule equals Equation 1 in https://arxiv.org/1909.02659."
  },
  {
    "objectID": "main.html#general-result-1",
    "href": "main.html#general-result-1",
    "title": "Automatic differentation rules for linear algebra",
    "section": "General result",
    "text": "General result\nWe now discuss the general result for the singular value decomposition of a rectangular matrix \\(A \\in {\\mathbb{C}}^{m\\times n}\\), which potentially has a rank \\(r \\leq \\min(m,n)\\), and which is possibly truncated, i.e. for which we only compute \\(p\\leq r\\) (typically the largest) singular values and corresponding vectors. We start from the exact relations\n\\[\\begin{align}\nAV &= US,&A^\\dagger U &= VS\n\\end{align}\\]\nwith thus \\(U \\in {\\mathbb{C}}^{m \\times p}\\), \\(V \\in {\\mathbb{C}}^{n \\times p}\\) and \\(S \\in {\\mathbb{R}}_{\\geq 0}^{p \\times p}\\). For the forward derivatives, we obtain\n\\[\\begin{align}\n\\dot{A}V &= \\dot{U}S - A\\dot{V} + U\\dot{S}\\\\\n\\dot{A}^\\dagger U &= \\dot{V}S - A^\\dagger\\dot{U} + V\\dot{S}\n\\end{align}\\] where we now insert \\[\\begin{align}\n\\dot{U}=U\\dot{K}+\\dot{L},\\qquad\\dot{K}=-\\dot{K}^\\dagger,\\qquad U^\\dagger \\dot{L}=O\\\\\n\\dot{V}=U\\dot{M}+\\dot{N},\\qquad\\dot{L}=-\\dot{L}^\\dagger,\\qquad V^\\dagger \\dot{N}=O\n\\end{align}\\]\nProjecting the first equation onto \\(U^\\dagger\\) and the second onto \\(V^\\dagger\\), we obtain\n\\[\\begin{align}\nU^\\dagger \\dot{A}V &= \\dot{K}S - S\\dot{M} + \\dot{S}\\\\\nV^\\dagger \\dot{A}^\\dagger U &= \\dot{M}S - S\\dot{K} + \\dot{S}\n\\end{align}\\] which are the equations we had solved in the full rank case in the previous subsection.\nInstead, we now project the first equation onto \\((I- UU^\\dagger)\\) and the second on \\((I- VV^\\dagger)\\) in order to find\n\\[\\begin{align}\n(I- UU^\\dagger)\\dot{A}V &= \\dot{L}S - (I- UU^\\dagger)A\\dot{N}\\\\\n(I- VV^\\dagger)\\dot{A}^\\dagger U &= \\dot{N}S - (I- VV^\\dagger)A^\\dagger\\dot{L}\n\\end{align}\\] The best way to solve these equations is to treat them as a single Sylvester equation by rewriting it in block matrix form as\n\\[\\begin{align}\n\\begin{bmatrix}\n(I- UU^\\dagger)\\dot{A}V\\\\\n(I- VV^\\dagger)\\dot{A}^\\dagger U\n\\end{bmatrix} = \\begin{bmatrix} \\dot{L} \\\\ \\dot{N} \\end{bmatrix} S - \\begin{bmatrix} O& (I- UU^\\dagger)A \\\\\n(I- VV^\\dagger)A^\\dagger & O\\end{bmatrix} \\begin{bmatrix} \\dot{L} \\\\ \\dot{N} \\end{bmatrix} = -\\mathcal{S}_{\\begin{bmatrix} O& (I- UU^\\dagger)A \\\\\n(I- VV^\\dagger)A^\\dagger & O\\end{bmatrix}, S}\\left(\\begin{bmatrix} \\dot{L} \\\\ \\dot{N} \\end{bmatrix}\\right)\n\\end{align}\\]\nAs in the case of the partial Hermitian eigenvalue factorisation, the terms \\((I- UU^\\dagger)A\\dot{N}\\) and \\((I- VV^\\dagger)A^\\dagger\\dot{L}\\) could be simplified to \\(A\\dot{N}\\) and \\(A^\\dagger\\dot{L}\\) by exploiting that \\(V^\\dagger \\dot{N}=O\\) and \\(U^\\dagger \\dot{L}=O\\). However, it is exactly the presence of these extra projectors that will enforce this condition, and make that the Sylvester operator has a trivial null space and thus becomes invertible. While we could consider more general types of regularisation as in the eigenvalue case, we now asssume that \\(S\\) does not contain zeros on the diagonal, as we are considering a truncated SVD, which is used in cases where zero singular values are probably meant to be truncated away. Furthermore note that \\((I- UU^\\dagger)A=A(I- VV^\\dagger) = A-USV^\\dagger\\) so that the first matrix appearing in the Sylvester operator is also Hermitian, and let us introduce the notation\n\\[A_\\perp = (I- UU^\\dagger)A=A(I- VV^\\dagger) = A-USV^\\dagger = (I- UU^\\dagger)A(I- VV^\\dagger)\\]\nWe thus obtain\n\\[\\begin{align}\n\\dot{S} &= \\mathcal{P}_{rD}(U^\\dagger \\dot{A}V) = \\mathcal{P}_{D}(\\mathcal{P}_{H}(U^\\dagger \\dot{A}V))\\\\\n\\dot{U} &= U \\dot{K} + \\dot{L}\\\\\n\\dot{V} &= U \\dot{M} + \\dot{N}\\\\\n\\dot{K} &= F\\odot \\mathcal{P}_{H}(U^\\dagger \\dot{A}V) + G\\odot \\mathcal{P}_{A}(U^\\dagger \\dot{A}V)\\\\\n\\dot{M} &= F\\odot \\mathcal{P}_{H}(U^\\dagger \\dot{A}V) - G\\odot \\mathcal{P}_{A}(U^\\dagger \\dot{A}V)\\\\\n\\begin{bmatrix} \\dot{L} \\\\ \\dot{N} \\end{bmatrix} &= -\\mathcal{S}^{-1}_{\\begin{bmatrix} O& A_\\perp \\\\\nA_\\perp^\\dagger & O\\end{bmatrix},S}\\left(\\begin{bmatrix}\n(I- UU^\\dagger)\\dot{A}V\\\\\n(I- VV^\\dagger)\\dot{A}^\\dagger U\n\\end{bmatrix}\\right)\n\\end{align}\\]\nwith\n\\[\\begin{align}\nF_{ij}&=\\begin{cases}\\frac{1}{s_j - s_i},& i\\neq j\\\\0,&\\text{otherwise}\\end{cases},\\\\\nG_{ij} &= \\frac{1}{s_i+s_j}.\n\\end{align}\\]\nas before. To formulate the reverse rule, we will need the quantities\n\\[\\begin{align}\n\\begin{bmatrix} \\breve{X} \\\\ \\breve{Y} \\end{bmatrix} &= -\\mathcal{S}^{-1}_{\\begin{bmatrix} O& A_\\perp \\\\\nA_\\perp^\\dagger & O\\end{bmatrix},S}\\left(\\begin{bmatrix}\n(I- UU^\\dagger)\\breve{U}\\\\\n(I- VV^\\dagger)\\breve{V}\n\\end{bmatrix}\\right)\n\\end{align}\\]\nso that we can now write\n\\[\\begin{align}\n\\mathop{\\mathrm{RTr}}(\\breve{U}^\\dagger \\dot{U})&+\\mathop{\\mathrm{RTr}}(\\breve{V}^\\dagger \\dot{V})+\\mathop{\\mathrm{RTr}}(\\breve{S}^\\dagger \\dot{S})\\\\\n&= \\mathop{\\mathrm{RTr}}(((U^\\dagger\\breve{U} + V^\\dagger\\breve{V})\\odot F)^\\dagger \\mathcal{P}_H(U^\\dagger \\dot{A}V)) \\\\\n&\\qquad + \\mathop{\\mathrm{RTr}}(((U^\\dagger\\breve{U} - V^\\dagger\\breve{V})\\odot G)^\\dagger \\mathcal{P}_A(U^\\dagger \\dot{A}V))\\\\\n&\\qquad+ \\mathop{\\mathrm{RTr}}(\\mathcal{P}_{rD}(\\breve{S})^\\dagger (U^\\dagger\\dot{A}V))\\\\\n&\\qquad +\\mathop{\\mathrm{RTr}}(\\breve{X}^\\dagger (I-UU^\\dagger) \\dot{A}V)\\\\\n&\\qquad +\\mathop{\\mathrm{RTr}}(\\breve{Y}^\\dagger (I-VV^\\dagger) \\dot{A}^\\dagger U)\\\\\n&= \\mathop{\\mathrm{RTr}}((U [ \\mathcal{P}_{rD}(\\breve{S}) + \\mathcal{P}_H((U^\\dagger\\breve{U} + V^\\dagger\\breve{V})\\odot F) + \\mathcal{P}_A((U^\\dagger\\breve{U} - V^\\dagger\\breve{V})\\odot G)]V^\\dagger)^\\dagger\\dot{A})\\\\\n&\\qquad+\\mathop{\\mathrm{RTr}}([(I- UU^\\dagger)\\breve{X}V^\\dagger]^\\dagger \\dot{A}) + \\mathop{\\mathrm{RTr}}([(I- VV^\\dagger)\\breve{Y}U^\\dagger]^\\dagger \\dot{A}^\\dagger)\n\\end{align}\\] from which we conclude that\n\\[\\begin{align}\n\\breve{A} &= U [ \\mathcal{P}_{rD}(\\breve{S}) + \\mathcal{P}_H((U^\\dagger\\breve{U} + V^\\dagger\\breve{V})\\odot F) + \\mathcal{P}_A((U^\\dagger\\breve{U} - V^\\dagger\\breve{V})\\odot G)]V^\\dagger\\\\\n&\\qquad + (I- UU^\\dagger)\\breve{X}V^\\dagger  + U\\breve{Y}^\\dagger (I- VV^\\dagger)\\\\\n&= U [ \\mathcal{P}_{rD}(\\breve{S}) + \\mathcal{P}_A(U^\\dagger\\breve{U} + V^\\dagger\\breve{V})\\odot F + \\mathcal{P}_A(U^\\dagger\\breve{U} - V^\\dagger\\breve{V})\\odot G]V^\\dagger\\\\\n&\\qquad + (I- UU^\\dagger)\\breve{X}V^\\dagger  + U\\breve{Y}^\\dagger (I- VV^\\dagger).\n\\end{align}\\]\nNote that \\(U^\\dagger \\breve{X}=O\\) and \\(V^\\dagger\\breve{Y}=O\\) should automatically be satisfied by the solutions \\((\\breve{X},\\breve{Y}\\) of the Sylvester equation, and the inclusion of the extra projectors onto the orthogonal complement of \\(U\\) and \\(V\\) in the terms containing \\(\\breve{X}\\) and \\(\\breve{Y}\\) is therefore redundant.\nIf we obtained the truncated SVD from first performing a full SVD, and thus have the orthogonal complements \\(U_\\perp\\) and \\(V_\\perp\\) available, we can rewrite this\n\\[\\begin{align}\n\\dot{S} &= \\mathcal{P}_{rD}(U^\\dagger \\dot{A}V) = \\mathcal{P}_{D}(\\mathcal{P}_{H}(U^\\dagger \\dot{A}V))\\\\\n\\dot{U} &= U \\dot{K} + U_\\perp \\dot{K}_\\perp\\\\\n\\dot{V} &= U \\dot{M} + V_\\perp \\dot{M}_\\perp\\\\\n\\dot{K} &= F\\odot \\mathcal{P}_{H}(U^\\dagger \\dot{A}V) + G\\odot \\mathcal{P}_{A}(U^\\dagger \\dot{A}V)\\\\\n\\dot{M} &= F\\odot \\mathcal{P}_{H}(U^\\dagger \\dot{A}V) - G\\odot \\mathcal{P}_{A}(U^\\dagger \\dot{A}V)\\\\\n\\begin{bmatrix} \\dot{K}_\\perp \\\\ \\dot{M}_\\perp \\end{bmatrix} &= -\\mathcal{S}^{-1}_{\\begin{bmatrix} O& U_\\perp^\\dagger AV_\\perp \\\\\nV_\\perp^\\dagger \\dot{A}^\\dagger U_\\perp & O\\end{bmatrix},S}\\left(\\begin{bmatrix}\nU_\\perp^\\dagger\\dot{A}V\\\\\nV_\\perp^\\dagger\\dot{A}^\\dagger U\n\\end{bmatrix}\\right)\n\\end{align}\\]\nand thus for the reverse rule\n\\[\\begin{align}\n\\breve{A} &= U [ \\mathcal{P}_{rD}(\\breve{S}) + \\mathcal{P}_H((U^\\dagger\\breve{U} + V^\\dagger\\breve{V})\\odot F) + \\mathcal{P}_A((U^\\dagger\\breve{U} - V^\\dagger\\breve{V})\\odot G)]V^\\dagger\\\\\n&\\qquad + U_\\perp \\breve{X}'V^\\dagger  + U\\breve{Y}'^\\dagger V_\\perp^\\dagger\\\\\n&= U [ \\mathcal{P}_{rD}(\\breve{S}) + \\mathcal{P}_A(U^\\dagger\\breve{U} + V^\\dagger\\breve{V})\\odot F + \\mathcal{P}_A(U^\\dagger\\breve{U} - V^\\dagger\\breve{V})\\odot G]V^\\dagger\\\\\n&\\qquad + U_\\perp \\breve{X}'V^\\dagger  + U\\breve{Y}'^\\dagger V_\\perp^\\dagger\\\\\n\\end{align}\\]\nwhere now \\(\\breve{X}'=U_\\perp^\\dagger \\breve{X}\\) and \\(\\breve{Y}' =V_\\perp^\\dagger \\breve{Y}\\) (running out of letters) can directly be obtained from\n\\[\\begin{align}\n\\begin{bmatrix} \\breve{X}' \\\\ \\breve{Y}' \\end{bmatrix} &= -\\mathcal{S}^{-1}_{\\begin{bmatrix} O& U_\\perp^\\dagger AV_\\perp \\\\\nV_\\perp^\\dagger \\dot{A}^\\dagger U_\\perp & O\\end{bmatrix},S}\\left(\\begin{bmatrix}\nU_\\perp^\\dagger\\breve{U}\\\\\nV_\\perp^\\dagger\\breve{V}\n\\end{bmatrix}\\right)\n\\end{align}\\]\nor thus, writing out the sylvester equation explicitly,\n\\[\\begin{align}\n\\breve{X}' S - U_\\perp^\\dagger AV_\\perp \\breve{Y}' &= U_\\perp^\\dagger\\breve{U}\\\\\n\\breve{Y}'S - V_\\perp^\\dagger \\dot{A}^\\dagger U_\\perp \\breve{X}' &= V_\\perp^\\dagger\\breve{V}\n\\end{align}\\]\nIf \\(U_\\perp\\) and \\(V_\\perp\\) are not just any orthogonal complement of \\(U\\) and \\(V\\), but truly come from a full SVD computation, it furthermore holds that \\(U_\\perp^\\dagger AV_\\perp\\) will also be diagonalised. Note that \\(U_\\perp \\in {\\mathbb{C}}^{m \\times (m-p)}\\) and \\(V \\in {\\mathbb{C}}^{n \\times (n-p)}\\), whereas \\(\\breve{X}' \\in {\\mathbb{C}}^{(m-p) \\times p}\\) en \\(\\breve{V}' \\in {\\mathbb{C}}^{(n-p)\\times p}\\). The matrix \\(U_\\perp^\\dagger AV_\\perp\\) is thus a \\((m-p) \\times (n-p)\\) matrix with nonzero elements only on the diagonal, namely \\((U_\\perp^\\dagger AV_\\perp)_{ij} = s_{i+p} \\delta_{ij}\\) with \\(\\{s_i, i=1,\\ldots,\\min(m,n)\n\\}\\) the list of all singular values of \\(A\\), and \\(\\{s_i,i=1,\\ldots,p\\}\\) the singular values contained in \\(S\\).\nWe now find the componentwise equations\n\\[\\begin{align}\n\\breve{X}'_{ik} s_k - s_{i+p} \\breve{Y}'_{ik} &= (U_\\perp^\\dagger \\breve{U})_{ik}\\\\\n\\breve{Y}'_{jk} s_k - s_{j+p} \\breve{X}'_{ik} &= (V_\\perp^\\dagger \\breve{V})_{jk}\n\\end{align}\\] with \\(i = 1,\\ldots, m-p\\), \\(j=1,\\ldots,n-p\\) and \\(k=1,\\ldots,p\\). Note that if \\(m \\neq n\\), we are already stretching the notation here, since the index of \\(s\\) might be going out of bounds in the second term (which we interpret as the corresponding \\(s\\) being zero). To solve this in way similar to the full SVD case for the square matrix, we first have to define a common square region for \\(i\\) and \\(j\\). Now, if there are only \\(r \\leq \\min(m,n)\\) nonzero singular values (\\(r\\) being the rank of \\(A\\)) we can separate these equations into\n\\[\\begin{align}\n\\breve{X}'_{ik} s_k - s_{i+p} \\breve{Y}'_{ik} &= (U_\\perp^\\dagger \\breve{U})_{ik}\\\\\n\\breve{Y}'_{jk} s_k - s_{j+p} \\breve{X}'_{jk} &= (V_\\perp^\\dagger \\breve{V})_{jk}\n\\end{align}\\]\nfor the square region \\(i,j = 1,\\ldots, r-p\\) and \\(k=1,\\ldots,p\\), whereas\n\\[\\begin{align}\n\\breve{X}'_{ik} s_k &= (U_\\perp^\\dagger \\breve{U})_{ik}\\\\\n\\breve{Y}'_{jk} s_k &= (V_\\perp^\\dagger \\breve{V})_{jk}\n\\end{align}\\]\nfor the remaining values \\(i=r-p+1,\\ldots,m-p\\) and \\(j=r-p+1,\\ldots,n-p\\), again for all \\(k=1,\\ldots,p\\).\nThis can now easily be solved to\n\\[\\begin{align}\n\\breve{X}'_{ik} &= \\frac{(U_\\perp^\\dagger \\breve{U})_{ik} s_k + s_{i+p} (V_\\perp^\\dagger \\breve{V})_{ik}}{s_k^2 - s_{i+p}^2}= \\frac{1}{2} \\frac{(U_\\perp^\\dagger \\breve{U})_{ik} + (V_\\perp^\\dagger \\breve{V})_{ik}}{s_k - s_{i+p}} + \\frac{1}{2}\\frac{(U_\\perp^\\dagger \\breve{U})_{ik} - (V_\\perp^\\dagger \\breve{V})_{ik}}{s_k + s_{i+p}}\\\\\n\\breve{Y}'_{ik} &= \\frac{(V_\\perp^\\dagger \\breve{V})_{ik} s_k + s_{i+p} (U_\\perp^\\dagger \\breve{U})_{ik}}{s_k^2 - s_{i+p}^2}=\\frac{1}{2} \\frac{(U_\\perp^\\dagger \\breve{U})_{ik} + (V_\\perp^\\dagger \\breve{V})_{ik}}{s_k - s_{i+p}} - \\frac{1}{2}\\frac{(U_\\perp^\\dagger \\breve{U})_{ik} - (V_\\perp^\\dagger \\breve{V})_{ik}}{s_k + s_{i+p}}\\\\\n\\end{align}\\] for \\(i = 1,\\ldots, r-p\\) and \\(k=1,\\ldots,p\\), and \\[\\begin{align}\n\\breve{X}'_{ik} &= \\frac{(U_\\perp^\\dagger \\breve{U})_{ik}}{s_k}\\\\\n\\breve{Y}'_{jk} &= \\frac{(V_\\perp^\\dagger \\breve{V})_{jk}}{s_k}\n\\end{align}\\]\nfor the remaining values \\(i=r-p+1,\\ldots,m-p\\) and \\(j=r-p+1,\\ldots,n-p\\), again for all \\(k=1,\\ldots,p\\)."
  },
  {
    "objectID": "main.html#alternative-from-svd",
    "href": "main.html#alternative-from-svd",
    "title": "Automatic differentation rules for linear algebra",
    "section": "Alternative from SVD",
    "text": "Alternative from SVD\nGiven that the polar decomposition is often computed from the SVD as \\[\\begin{align}\nW &= U V^\\dagger & P &= V S V^\\dagger\n\\end{align}\\] we also find \\[\\begin{align}\n\\dot{W} &= \\dot{U} V^\\dagger + U \\dot{V}^\\dagger\\\\\nP &= \\dot{V} S V^\\dagger + V \\dot{S} V^\\dagger + V S \\dot{V}^\\dagger\n\\end{align}\\]\nWe can then write \\[\\begin{align}\n\\mathop{\\mathrm{RTr}}&(\\breve{W}^\\dagger \\dot{W}) + \\mathop{\\mathrm{RTr}}(\\mathcal{P}_H(\\breve{P})^\\dagger \\dot{P}) =\\\\\n&\\mathop{\\mathrm{RTr}}((\\breve{W}V)^\\dagger \\dot{U}) + \\mathop{\\mathrm{RTr}}((U^\\dagger \\breve{W})^\\dagger \\dot{V}^\\dagger) +\\\\\n& \\mathop{\\mathrm{RTr}}((\\mathcal{P}_H(\\breve{P})V S)^\\dagger \\dot{V}) + \\mathop{\\mathrm{RTr}}((V^\\dagger \\mathcal{P}_H(\\breve{P})V )^\\dagger \\dot{S}) + \\mathop{\\mathrm{RTr}}((SV^\\dagger \\mathcal{P}_H(\\breve{P}) )^\\dagger\\dot{V}^\\dagger)\n\\end{align}\\] which can be rewritten as \\[\\begin{align}\n\\mathop{\\mathrm{RTr}}&((\\breve{W}V)^\\dagger \\dot{U}) + \\mathop{\\mathrm{RTr}}(\\mathcal{P}_D(V^\\dagger \\mathcal{P}_H(\\breve{P})V )^\\dagger \\dot{S}) + \\mathop{\\mathrm{RTr}}((2\\mathcal{P}_H(\\breve{P})VS+ \\breve{W}^\\dagger U)^\\dagger \\dot{V}).\n\\end{align}\\] Indeed, this is exactly what AD would do to pull back the polar cotangents \\((\\breve{W},\\breve{P})\\) to SVD cotangents \\[\\begin{align}\n\\breve{U} &= \\breve{W} V\\\\\n\\breve{S} &= \\mathcal{P}_D(V^\\dagger \\mathcal{P}_H(\\breve{P})V )\\\\\n\\breve{V} &= 2\\mathcal{P}_H(\\breve{P})VS+ \\breve{W}^\\dagger U\n\\end{align}\\]"
  }
]